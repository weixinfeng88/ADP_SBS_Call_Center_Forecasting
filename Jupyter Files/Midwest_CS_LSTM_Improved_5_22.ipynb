{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ea3fd7-c181-4688-8062-eeae480b1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 08:03:04.366238: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 08:03:04.369777: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 08:03:04.380255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748246584.397786     400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748246584.403817     400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748246584.417075     400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246584.417087     400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246584.417089     400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246584.417091     400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-26 08:03:04.421514: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "from datetime import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b2d2c2-248d-4cf0-94c3-69db4548d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    计算 MAPE（平均绝对百分比误差）\n",
    "\n",
    "    参数:\n",
    "    \n",
    "    \n",
    "    actual (array-like): 实际值数组。\n",
    "    predicted (array-like): 预测值数组。\n",
    "\n",
    "    返回:\n",
    "    float: MAPE 值。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 numpy 数组\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 避免除以零的情况\n",
    "    if np.any(actual == 0):\n",
    "        raise ValueError(\"实际值中包含零，无法计算 MAPE。\")\n",
    "    # 计算 MAPE\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49150e8-6435-4652-830c-8aed2128f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the interval need to forecast here\n",
    "unique_times = [ datetime.time(10, 0),\n",
    "datetime.time(10, 30), datetime.time(11, 0), datetime.time(11, 30),\n",
    "datetime.time(12, 0), datetime.time(12, 30), datetime.time(13, 0),\n",
    "datetime.time(13, 30), datetime.time(14, 0), datetime.time(14, 30),\n",
    "datetime.time(15, 0), datetime.time(15, 30), datetime.time(16, 0),\n",
    "datetime.time(16, 30), datetime.time(17, 0), datetime.time(17, 30),datetime.time(18,0),datetime.time(18,30),datetime.time(19,0),datetime.time(19,30),datetime.time(20,0),datetime.time(20,30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa5ef03-b7d0-4f39-8f6e-4ff3a395bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3999269/72552902.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
      "/tmp/ipykernel_3999269/72552902.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['date'] = data.datetime.dt.date\n",
      "/tmp/ipykernel_3999269/72552902.py:25: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_3999269/72552902.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_3999269/72552902.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_3999269/72552902.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_3999269/72552902.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['time'] = data.datetime.dt.time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data = pd.read_excel('workspace/MidWest_WFM_Stat_2025_05_16.xlsx')\n",
    "data = pd.read_excel('MidWest_WFM_Stat_2025_05_16.xlsx')\n",
    "pd.set_option('display.max_rows', 100)\n",
    "data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "       'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "       'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "\n",
    "data = data[~data .offered.isna()]\n",
    "\n",
    "data = data[data .offered != 0]\n",
    "# Month one and half need to set up \n",
    "data_=data\n",
    "data = data[data.conversation_start_interval_tmst<=pd.to_datetime('{} 00:00:00'.format('2025-01-15'))]\n",
    "#holiday_data = pd.read_excel(r'workspace/holiday.xlsx')\n",
    "holiday_data = pd.read_excel(r'holiday.xlsx')\n",
    "holiday_data['~is_holiday'] = 0\n",
    "Holiday_name = ['Christmas Day', 'Columbus Day',\n",
    "       'Independence Day', 'Labor Day', 'Martin Luther King Jr. Day',\n",
    "       'Memorial Day', \"New Year's Day\", \"Presidents' Day\", 'Thanksgiving Day',\n",
    "       'Veterans Day']\n",
    "holiday_data['date'] = holiday_data.Date.dt.date\n",
    "data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
    "data['date'] = data.datetime.dt.date\n",
    "\n",
    "data.datetime = data.datetime.dt.floor('T')\n",
    "data['datetime'] = data['datetime'].apply(\n",
    "    lambda x: x.ceil('30T') if x.minute == 29 else x\n",
    ")\n",
    "data['datetime'] = data['datetime'].apply(\n",
    "    lambda x: x.ceil('H') if x.minute == 59 else x\n",
    ")\n",
    "data['time'] = data.datetime.dt.time\n",
    "data = data.sort_values('datetime').reset_index(drop = True)\n",
    "data = data.sort_values(by='datetime')\n",
    "\n",
    "# 获取所有唯一的时间点（如每天的09:00, 09:30等）\n",
    "#unique_times = data['time'].unique()\n",
    "\n",
    "# 获取所有唯一的日期\n",
    "unique_dates = data['date'].unique()\n",
    "data = pd.merge(data,holiday_data,on = 'date',how = 'left').fillna(1)\n",
    "data = data[data['~is_holiday'] == 1]\n",
    "\n",
    "holiday_datetime = holiday_data.date.to_numpy()\n",
    "\n",
    "start_time = 35+23 # 58 how many days in advance\n",
    "end_time = start_time+36 #36 is the train length\n",
    "\n",
    "\n",
    "add_column = ['actans','actabn','absActHt','absActSa']\n",
    "\n",
    "add_columns =  [f'actans_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'actabn_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'absActHt_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'absActSa_{i}' for i in range(start_time+1,end_time+1) ]\n",
    "\n",
    "matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fe826c-09a3-4751-8672-b58f481e937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1dfb2df-7072-43b2-97cb-629ae74e542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "\n",
    "# 遍历每个时间点\n",
    "for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "    # 过滤出当前时间点的数据\n",
    "    time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "    full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "    full_dates = full_dates.difference(holiday_datetime)\n",
    "    time_data = time_data.groupby(time_data.index).sum()\n",
    "    time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "    shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "    \n",
    "    \n",
    "    add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "    add_data = add_data.groupby(add_data.index).sum()\n",
    "    add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "    add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "    \n",
    "    \n",
    "    time_data1 = data[data['time'] == time1].set_index('date')['offered']    \n",
    "    time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "    time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "    shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "    \n",
    "    time_data2 = data[data['time'] == time2].set_index('date')['offered']    \n",
    "    time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "    time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "    shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "    df=time_data.to_frame(name='offer')\n",
    "    df_last_year = df.copy()\n",
    "    df_last_year.index = df_last_year.index + pd.DateOffset(years=1)\n",
    "    df_last_year = df_last_year.rename(columns={'offer': 'offered_last_year'}) \n",
    "    # Step 3: Join on index\n",
    "    result = df.join(df_last_year, how='left')\n",
    "    year_data=result['offered_last_year']  \n",
    "    #year_data = time_data.shift(251)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27db9b80-83e8-4773-899f-15f127aed31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data=time_data.copy()\n",
    "year_data.index = year_data.index + pd.DateOffset(years=1)\n",
    "year_data.columns=['last+year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee6df619-368e-461c-9fa0-068184f19d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-12-21    114.0\n",
       "2022-12-22    110.0\n",
       "2022-12-23    102.0\n",
       "2022-12-24     33.0\n",
       "2022-12-27    102.0\n",
       "              ...  \n",
       "2026-01-08    128.0\n",
       "2026-01-09    127.0\n",
       "2026-01-10    137.0\n",
       "2026-01-13    154.0\n",
       "2026-01-14    161.0\n",
       "Name: offered, Length: 772, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b7e7e9b-c79d-4a6c-89d7-f85f1c982654",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3999269/32532010.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Step 1: Shift the index by 1 year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_data_last_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime_data_last_year\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_data_last_year\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Step 2: Join the original with the shifted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_data_last_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "# Assume time_data has datetime index and 'offered' column\n",
    "# Step 1: Shift the index by 1 year\n",
    "time_data_last_year = time_data.copy()\n",
    "time_data_last_year.index = time_data_last_year.index + pd.DateOffset(years=1)\n",
    "# Step 2: Join the original with the shifted\n",
    "result = time_data.join(time_data_last_year, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de3c9b5a-8ea4-4c58-aa4d-12e4599d17f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4d2588e-dca6-489c-8833-18ce7b36f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data_=time_data.to_frame(name='offer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a120e13a-7cc5-4906-817b-a56d12b9d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=time_data.to_frame(name='offer')\n",
    "df_last_year = df.copy()\n",
    "df_last_year.index = df_last_year.index + pd.DateOffset(years=1)\n",
    "df_last_year = df_last_year.rename(columns={'offer': 'offered_last_year'})\n",
    "\n",
    "# Step 3: Join on index\n",
    "result = df.join(df_last_year, how='left')\n",
    "year_data=result['offered_last_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d62f6a0-722d-4194-adef-05759d979cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(420)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['offered_last_year'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae8aa040-8bc1-4eae-a445-7235c4450ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['offered_last_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c1c3444-e7d1-4771-8c9d-1c6950d0affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data=result['offered_last_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20d21f56-dee3-4ec8-b3a2-1123844d4139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f13fd3d1-08ed-49fe-af75-3b007484f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021-12-21      NaN\n",
       "2021-12-22      NaN\n",
       "2021-12-23      NaN\n",
       "2021-12-24      NaN\n",
       "2021-12-27      NaN\n",
       "              ...  \n",
       "2025-01-08    120.0\n",
       "2025-01-09    160.0\n",
       "2025-01-10    114.0\n",
       "2025-01-13      NaN\n",
       "2025-01-14      NaN\n",
       "Name: offered_last_year, Length: 772, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7712ca4-4dca-4bf1-817b-24d20c8d71d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "your_venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
