{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 08:06:32.752683: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 08:06:32.756108: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 08:06:32.766612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748246792.784008     581 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748246792.789285     581 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748246792.802313     581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246792.802328     581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246792.802329     581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748246792.802331     581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-26 08:06:32.806683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    计算 MAPE（平均绝对百分比误差）\n",
    "\n",
    "    参数:\n",
    "    \n",
    "    \n",
    "    actual (array-like): 实际值数组。\n",
    "    predicted (array-like): 预测值数组。\n",
    "\n",
    "    返回:\n",
    "    float: MAPE 值。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 numpy 数组\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 避免除以零的情况\n",
    "    if np.any(actual == 0):\n",
    "        raise ValueError(\"实际值中包含零，无法计算 MAPE。\")\n",
    "\n",
    "    # 计算 MAPE\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('CS South WFM STAT 2025_05_14.xlsx')\n",
    "pd.set_option('display.max_rows', 100)\n",
    "data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "       'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "       'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "\n",
    "data = data[~data .offered.isna()]\n",
    "\n",
    "data = data[data .offered != 0]\n",
    "# Month one and half need to set up \n",
    "data = data[data['conversation_start_interval_tmst']<=pd.to_datetime('{} 00:00:00'.format('2025-01-15'))]\n",
    "holiday_data = pd.read_excel(r'holiday.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-01-14 18:30:00')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['conversation_start_interval_tmst'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224728/4027003370.py:24: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1747145764.140436  224728 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1747145764.166294  224728 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - loss: 0.2456\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 226ms/step - loss: 0.0333\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - loss: 0.0277\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - loss: 0.0250\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - loss: 0.0241\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0240\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0252\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 0.0267\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 0.0240\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0241\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step - loss: 0.0251\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - loss: 0.0246\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - loss: 0.0237\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - loss: 0.0247\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - loss: 0.0236\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - loss: 0.0187\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - loss: 0.0251\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 226ms/step - loss: 0.0240\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0217\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - loss: 0.0267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step\n",
      "21.3801797887888\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - loss: 0.4898\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 210ms/step - loss: 0.0486\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - loss: 0.0342\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0241\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 0.0250\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 0.0235\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 226ms/step - loss: 0.0247\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 0.0256\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 0.0250\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 0.0232\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 0.0220\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - loss: 0.0226\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 0.0256\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0234\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - loss: 0.0224\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - loss: 0.0265\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0266\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - loss: 0.0233\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0235\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 0.0241\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step\n",
      "18.428797893793362\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "3854.178234191445\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - loss: 0.2374\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 0.0605\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 0.0321\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - loss: 0.0295\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step - loss: 0.0259\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - loss: 0.0259\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0247\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - loss: 0.0237\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 210ms/step - loss: 0.0231\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 0.0248\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 0.0271\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - loss: 0.0254\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 0.0245\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 0.0239\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step - loss: 0.0257\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - loss: 0.0260\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 224ms/step - loss: 0.0257\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0275\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - loss: 0.0221\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - loss: 0.0245\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n",
      "15.888861391496373\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199ms/step - loss: 0.0756\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0335\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 0.0308\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0245\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - loss: 0.0279\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - loss: 0.0249\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - loss: 0.0254\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - loss: 0.0254\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - loss: 0.0266\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0293\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - loss: 0.0270\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - loss: 0.0251\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - loss: 0.0256\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0252\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0242\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - loss: 0.0249\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - loss: 0.0241\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - loss: 0.0237\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - loss: 0.0254\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - loss: 0.0232\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step\n",
      "16.780240533826372\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "6904.555258863312\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 214ms/step - loss: 0.0614\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - loss: 0.0241\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 191ms/step - loss: 0.0251\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - loss: 0.0233\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - loss: 0.0236\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step - loss: 0.0244\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 189ms/step - loss: 0.0225\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0234\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0217\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - loss: 0.0225\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - loss: 0.0210\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - loss: 0.0215\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 205ms/step - loss: 0.0236\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step - loss: 0.0222\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - loss: 0.0216\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - loss: 0.0226\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0207\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - loss: 0.0193\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 0.0198\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - loss: 0.0237\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step\n",
      "18.5237391371248\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - loss: 0.1008\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - loss: 0.0362\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 201ms/step - loss: 0.0236\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 189ms/step - loss: 0.0219\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step - loss: 0.0232\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0226\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0239\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step - loss: 0.0229\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0238\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step - loss: 0.0221\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - loss: 0.0205\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - loss: 0.0226\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - loss: 0.0222\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 207ms/step - loss: 0.0229\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - loss: 0.0234\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 187ms/step - loss: 0.0216\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - loss: 0.0235\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - loss: 0.0238\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - loss: 0.0254\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step - loss: 0.0226\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n",
      "16.239881966385195\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "7051.743977717256\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - loss: 0.0388\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0217\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - loss: 0.0198\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 0.0193\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - loss: 0.0191\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - loss: 0.0185\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - loss: 0.0188\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0172\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - loss: 0.0161\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 0.0166\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 195ms/step - loss: 0.0166\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - loss: 0.0167\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - loss: 0.0172\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - loss: 0.0166\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - loss: 0.0155\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 197ms/step - loss: 0.0181\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step - loss: 0.0155\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 231ms/step - loss: 0.0167\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 209ms/step - loss: 0.0172\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - loss: 0.0192\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step\n",
      "16.8996403873426\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - loss: 0.2716\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 195ms/step - loss: 0.0327\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 201ms/step - loss: 0.0247\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - loss: 0.0216\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0192\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 184ms/step - loss: 0.0159\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0179\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - loss: 0.0182\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - loss: 0.0210\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0159\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 195ms/step - loss: 0.0175\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - loss: 0.0171\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 0.0156\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step - loss: 0.0172\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.0186\n",
      "Epoch 16/20\n",
      "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 255ms/step - loss: 0.0225"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 352\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# 编译模型\u001b[39;00m\n\u001b[1;32m    351\u001b[0m model_2\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 352\u001b[0m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m train_predict_2 \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mpredict(X_train_2)\n\u001b[1;32m    354\u001b[0m train_predict_2 \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(train_predict_2)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for month_number in ['2024-11-15']:\n",
    "\n",
    "#for month_number in ['2024-11-15','2024-12-15','2025-1-15','2025-2-15','2025-3-15']:\n",
    "    #ETL Process\n",
    "    data = pd.read_excel('CS South WFM STAT 2025_05_14.xlsx')\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "           'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "           'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "    \n",
    "    data = data[~data .offered.isna()]\n",
    "    \n",
    "    data = data[data .offered != -1]\n",
    "    # Month one and half need to set up \n",
    "    data = data[data.conversation_start_interval_tmst<=pd.to_datetime('{} 00:00:00'.format(month_number))]\n",
    "    holiday_data = pd.read_excel(r'holiday.xlsx')\n",
    "    holiday_data['~is_holiday'] = 0\n",
    "    Holiday_name = ['Christmas Day', 'Columbus Day',\n",
    "           'Independence Day', 'Labor Day', 'Martin Luther King Jr. Day',\n",
    "           'Memorial Day', \"New Year's Day\", \"Presidents' Day\", 'Thanksgiving Day',\n",
    "           'Veterans Day']\n",
    "    holiday_data['date'] = holiday_data.Date.dt.date\n",
    "    data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
    "    data['date'] = data.datetime.dt.date\n",
    "    \n",
    "    data.datetime = data.datetime.dt.floor('T')\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('30T') if x.minute == 29 else x\n",
    "    )\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('H') if x.minute == 59 else x\n",
    "    )\n",
    "    data['time'] = data.datetime.dt.time\n",
    "    data = data.sort_values('datetime').reset_index(drop = True)\n",
    "    data = data.sort_values(by='datetime')\n",
    "    \n",
    "    # 获取所有唯一的时间点（如每天的09:00, 09:30等）\n",
    "    unique_times = data['time'].unique()\n",
    "    \n",
    "    # 获取所有唯一的日期\n",
    "    unique_dates = data['date'].unique()\n",
    "    data = pd.merge(data,holiday_data,on = 'date',how = 'left').fillna(1)\n",
    "    data = data[data['~is_holiday'] == 1]\n",
    "    \n",
    "    holiday_datetime = holiday_data.date.to_numpy()\n",
    "    \n",
    "    start_time = 35+23 # 58 how many days in advance\n",
    "    end_time = start_time+36 #36 is the train length\n",
    "    # set up the interval need to forecast here\n",
    "    unique_times = [ \n",
    "    time(9, 0), time(9, 30), time(10, 0),\n",
    "    time(10, 30), time(11, 0), time(11, 30),\n",
    "    time(12, 0), time(12, 30), time(13, 0),\n",
    "    time(13, 30), time(14, 0), time(14, 30),\n",
    "    time(15, 0), time(15, 30), time(16, 0),\n",
    "    time(16, 30), time(17, 0), time(17, 30)\n",
    "    ]\n",
    "    \n",
    "    add_column = ['actans','actabn','absActHt','absActSa']\n",
    "    \n",
    "    add_columns =  [f'actans_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'actabn_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActHt_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActSa_{i}' for i in range(start_time+1,end_time+1) ]\n",
    "    \n",
    "    matrix = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "        #full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max()+ pd.offsets.BDay(start_time), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()\n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered']    \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']    \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])\n",
    "        #combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])[-start_time:]\n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix)\n",
    "    \n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix[i] = np.vstack([matrix[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix = np.vstack(matrix)\n",
    "    \n",
    "    matrix = pd.DataFrame(matrix,columns = ['datetime','offered'] +add_columns +  [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + \\\n",
    "                          [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix['year'] = matrix.datetime.map(lambda x:x.year)\n",
    "    matrix['month'] = matrix.datetime.map(lambda x:x.month)\n",
    "    matrix['day'] = matrix.datetime.map(lambda x:x.day)\n",
    "    matrix['hour'] = matrix.datetime.map(lambda x:x.hour)\n",
    "    matrix['minute'] = matrix.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix = matrix.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data = matrix.iloc[:]\n",
    "    read_data = read_data[~read_data.offered.isna()].reset_index(drop = True)\n",
    "    \n",
    "    fill_na = read_data.iloc[:,1:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na = fill_na.interpolate(method='linear')\n",
    "    \n",
    "    fill_na['datetime'] = read_data.datetime\n",
    "    fill_na['date'] = fill_na.datetime.dt.date\n",
    "    fill_na = fill_na.dropna().reset_index(drop = True)\n",
    "    \n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['year'],prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na['week_of_year'] = fill_na['datetime'].dt.isocalendar().week\n",
    "    fill_na['weekday'] = fill_na['datetime'].dt.day_name()\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['weekday'],prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    matrix_predict = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max()+ pd.offsets.BDay(start_time), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()    \n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered'] \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']  \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])[-start_time:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix_predict.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix_predict)\n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix_predict)):\n",
    "        if matrix_predict[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix_predict[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix_predict[i] = np.vstack([matrix_predict[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix_predict = np.vstack(matrix_predict)\n",
    "    \n",
    "    matrix_predict = pd.DataFrame(matrix_predict,columns = ['datetime','offered'] + add_columns+ [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix_predict['year'] = matrix_predict.datetime.map(lambda x:x.year)\n",
    "    matrix_predict['month'] = matrix_predict.datetime.map(lambda x:x.month)\n",
    "    matrix_predict['day'] = matrix_predict.datetime.map(lambda x:x.day)\n",
    "    matrix_predict['hour'] = matrix_predict.datetime.map(lambda x:x.hour)\n",
    "    matrix_predict['minute'] = matrix_predict.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix_predict = matrix_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data_predict = matrix_predict.iloc[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na_predict = read_data_predict.iloc[:,2:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    fill_na_predict = fill_na_predict.interpolate(method='linear')\n",
    "    fill_na_predict = fill_na_predict.fillna(1)\n",
    "    \n",
    "    fill_na_predict['datetime'] = read_data_predict.datetime\n",
    "    fill_na_predict['date'] = fill_na_predict.datetime.dt.date\n",
    "    \n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['year'], prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na_predict['week_of_year'] = fill_na_predict['datetime'].dt.isocalendar().week\n",
    "    fill_na_predict['weekday'] = fill_na_predict['datetime'].dt.day_name()\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['weekday'], prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    fill_na_predict\n",
    "    \n",
    "    X_columns = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "                \n",
    "    X_columns2 = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "    \n",
    "    \n",
    "    lstm_data = fill_na.dropna(axis = 0)\n",
    "    \n",
    "    lstm_data\n",
    "    \n",
    "    for i in X_columns:\n",
    "        try:\n",
    "            fill_na_predict[i]\n",
    "        except:\n",
    "            fill_na_predict[i] = 0\n",
    "    \n",
    "    lstm_data = lstm_data[lstm_data.year>=2021].reset_index(drop = True)\n",
    "    \n",
    "    week_dict = {'Friday':5, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4}\n",
    "    \n",
    "    lstm_data['week_num'] = lstm_data['weekday'].map(lambda x:week_dict[x])\n",
    "    fill_na_predict['week_num'] = fill_na_predict['weekday'].map(lambda x:week_dict[x])\n",
    "\n",
    "    \n",
    "    all_predict = pd.DataFrame()\n",
    "    all_feature = pd.DataFrame()\n",
    "    #for week_ in [1,2,3,4,5]:\n",
    "    for (hour,minute) in lstm_data[['hour','minute']].drop_duplicates().sort_values(['hour','minute']).to_numpy()[:]:\n",
    "        now_lstm_data = lstm_data[(lstm_data.hour == hour)&(lstm_data.minute == minute)]\n",
    "        #now_lstm_data = now_lstm_data[now_lstm_data.week_num == week_]\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        #now_fill_na_predict = now_fill_na_predict[now_fill_na_predict.week_num == week_]\n",
    "    \n",
    "        now_time_list = now_lstm_data.datetime.to_numpy()\n",
    "        now_lstm_data = now_lstm_data[['offered']+['datetime'] +X_columns].reset_index(drop = True)\n",
    "        # 归一化数据\n",
    "    \n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "        X_train_1, y_train =now_lstm_data[X_columns][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_1, y_test =now_lstm_data[X_columns][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_1 = np.array(X_train_1)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_1 = np.array(X_test_1)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_1 = Sequential()\n",
    "        model_1.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_1.add(LSTM(50, return_sequences=False))\n",
    "        model_1.add(Dense(25))\n",
    "        model_1.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_1.fit(X_train_1, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_1 = model_1.predict(X_train_1)\n",
    "        train_predict_1 = scaler.inverse_transform(train_predict_1)\n",
    "        True_data_1 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_1, True_data_1))\n",
    "        return_y_1 =  model_1.predict(X_test_1)\n",
    "        return_y_pred_1 = scaler.inverse_transform(np.array(return_y_1).reshape(-1, 1))\n",
    "        now_predict_1 = pd.DataFrame(return_y_pred_1,columns = ['predict'])\n",
    "        now_predict_1['datetime'] = now_time_list[-start_time:]\n",
    "        now_lstm_data = now_lstm_data[['offered'] +X_columns2].reset_index(drop = True)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "    \n",
    "        X_train_2, y_train =now_lstm_data[X_columns2][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_2, y_test =now_lstm_data[X_columns2][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_2 = np.array(X_train_2)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_2 = np.array(X_test_2)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], X_test_2.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], X_train_2.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_2 = Sequential()\n",
    "        model_2.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_2.add(LSTM(50, return_sequences=False))\n",
    "        model_2.add(Dense(25))\n",
    "        model_2.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_2.fit(X_train_2, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_2 = model_2.predict(X_train_2)\n",
    "        train_predict_2 = scaler.inverse_transform(train_predict_2)\n",
    "        True_data_2 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_2, True_data_2))\n",
    "        return_y_2 =  model_2.predict(X_test_2)\n",
    "        return_y_pred_2 = scaler.inverse_transform(np.array(return_y_2).reshape(-1, 1))\n",
    "        now_predict_2 = pd.DataFrame(return_y_pred_2,columns = ['predict'])\n",
    "        now_predict_2['datetime'] = now_time_list[-start_time:]\n",
    "        print(calculate_mape(y_test, return_y_pred_2))\n",
    "        if calculate_mape(y_test, return_y_pred_1) < calculate_mape(y_test, return_y_pred_2):\n",
    "            all_predict = pd.concat([now_predict_1,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_1.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        else:\n",
    "            all_predict = pd.concat([now_predict_2,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns2].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_2.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        #print('Interval Hour:{}, Minute:{} has been trained'.format(hour,minute)\n",
    "    all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    all_feature.to_excel(r'return_month_{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'offered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2871/2965003655.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'offered'"
     ]
    }
   ],
   "source": [
    "all_predict['diff'] = (all_predict.predict - all_predict.offered)\n",
    "\n",
    "all_predict['hour'] = all_predict.datetime.dt.hour\n",
    "\n",
    "pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "\n",
    "calculate_mape(all_predict.offered,all_predict.predict)\n",
    "\n",
    "all_predict.to_excel(r'return{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>datetime</th>\n",
       "      <th>offered_x</th>\n",
       "      <th>diff</th>\n",
       "      <th>hour</th>\n",
       "      <th>offered_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.317352</td>\n",
       "      <td>2024-03-19 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.205730</td>\n",
       "      <td>2024-03-20 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.433464</td>\n",
       "      <td>2024-03-21 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.991905</td>\n",
       "      <td>2024-03-22 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.519444</td>\n",
       "      <td>2024-03-25 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>151.073898</td>\n",
       "      <td>2024-09-19 11:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.073898</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>152.045944</td>\n",
       "      <td>2024-09-19 13:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.045944</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>130.966293</td>\n",
       "      <td>2024-09-19 16:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.966293</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>85.822189</td>\n",
       "      <td>2024-09-19 16:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.822189</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>17.478544</td>\n",
       "      <td>2024-09-19 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.478544</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict            datetime  offered_x        diff  hour  offered_y\n",
       "0      55.317352 2024-03-19 17:30:00        NaN         NaN   NaN       52.0\n",
       "1      57.205730 2024-03-20 17:30:00        NaN         NaN   NaN       50.0\n",
       "2      57.433464 2024-03-21 17:30:00        NaN         NaN   NaN       75.0\n",
       "3      56.991905 2024-03-22 17:30:00        NaN         NaN   NaN       36.0\n",
       "4      56.519444 2024-03-25 17:30:00        NaN         NaN   NaN       81.0\n",
       "...          ...                 ...        ...         ...   ...        ...\n",
       "1097  151.073898 2024-09-19 11:00:00        1.0  150.073898  11.0        1.0\n",
       "1098  152.045944 2024-09-19 13:00:00        2.0  150.045944  13.0        2.0\n",
       "1099  130.966293 2024-09-19 16:00:00        2.0  128.966293  16.0        2.0\n",
       "1100   85.822189 2024-09-19 16:30:00        2.0   83.822189  16.0        2.0\n",
       "1101   17.478544 2024-09-19 17:00:00        1.0   16.478544  17.0        1.0\n",
       "\n",
       "[1102 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n"
     ]
    }
   ],
   "source": [
    "all_predict = pd.concat([now_predict_1,all_predict])\n",
    "future_X = now_fill_na_predict[X_columns].to_numpy()\n",
    "future_data = scaler.inverse_transform(np.array(\\\n",
    "                model_1.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "all_feature = pd.concat([all_feature,future_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>datetime</th>\n",
       "      <th>offered</th>\n",
       "      <th>diff</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.317352</td>\n",
       "      <td>2024-03-19 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.205730</td>\n",
       "      <td>2024-03-20 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.433464</td>\n",
       "      <td>2024-03-21 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.991905</td>\n",
       "      <td>2024-03-22 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.519444</td>\n",
       "      <td>2024-03-25 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>151.073898</td>\n",
       "      <td>2024-09-19 11:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.073898</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>152.045944</td>\n",
       "      <td>2024-09-19 13:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.045944</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>130.966293</td>\n",
       "      <td>2024-09-19 16:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.966293</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>85.822189</td>\n",
       "      <td>2024-09-19 16:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.822189</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>17.478544</td>\n",
       "      <td>2024-09-19 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.478544</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict            datetime  offered        diff  hour\n",
       "0      55.317352 2024-03-19 17:30:00      NaN         NaN   NaN\n",
       "1      57.205730 2024-03-20 17:30:00      NaN         NaN   NaN\n",
       "2      57.433464 2024-03-21 17:30:00      NaN         NaN   NaN\n",
       "3      56.991905 2024-03-22 17:30:00      NaN         NaN   NaN\n",
       "4      56.519444 2024-03-25 17:30:00      NaN         NaN   NaN\n",
       "...          ...                 ...      ...         ...   ...\n",
       "1039  151.073898 2024-09-19 11:00:00      1.0  150.073898  11.0\n",
       "1040  152.045944 2024-09-19 13:00:00      2.0  150.045944  13.0\n",
       "1041  130.966293 2024-09-19 16:00:00      2.0  128.966293  16.0\n",
       "1042   85.822189 2024-09-19 16:30:00      2.0   83.822189  16.0\n",
       "1043   17.478544 2024-09-19 17:00:00      1.0   16.478544  17.0\n",
       "\n",
       "[1102 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>datetime</th>\n",
       "      <th>offered_x</th>\n",
       "      <th>diff</th>\n",
       "      <th>hour</th>\n",
       "      <th>offered_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.317352</td>\n",
       "      <td>2024-03-19 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>55.317352</td>\n",
       "      <td>2024-03-19 17:30:00</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.317352</td>\n",
       "      <td>17.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>134.345093</td>\n",
       "      <td>2024-03-20 09:00:00</td>\n",
       "      <td>101.0</td>\n",
       "      <td>33.345093</td>\n",
       "      <td>9.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.205730</td>\n",
       "      <td>2024-03-20 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>57.205730</td>\n",
       "      <td>2024-03-20 17:30:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.205730</td>\n",
       "      <td>17.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>151.073898</td>\n",
       "      <td>2024-09-19 11:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.073898</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>152.045944</td>\n",
       "      <td>2024-09-19 13:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.045944</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>130.966293</td>\n",
       "      <td>2024-09-19 16:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.966293</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>85.822189</td>\n",
       "      <td>2024-09-19 16:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.822189</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>17.478544</td>\n",
       "      <td>2024-09-19 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.478544</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict            datetime  offered_x        diff  hour  offered_y\n",
       "0      55.317352 2024-03-19 17:30:00        NaN         NaN   NaN       52.0\n",
       "58     55.317352 2024-03-19 17:30:00       52.0    3.317352  17.0       52.0\n",
       "59    134.345093 2024-03-20 09:00:00      101.0   33.345093   9.0      101.0\n",
       "1      57.205730 2024-03-20 17:30:00        NaN         NaN   NaN       50.0\n",
       "60     57.205730 2024-03-20 17:30:00       50.0    7.205730  17.0       50.0\n",
       "...          ...                 ...        ...         ...   ...        ...\n",
       "1097  151.073898 2024-09-19 11:00:00        1.0  150.073898  11.0        1.0\n",
       "1098  152.045944 2024-09-19 13:00:00        2.0  150.045944  13.0        2.0\n",
       "1099  130.966293 2024-09-19 16:00:00        2.0  128.966293  16.0        2.0\n",
       "1100   85.822189 2024-09-19 16:30:00        2.0   83.822189  16.0        2.0\n",
       "1101   17.478544 2024-09-19 17:00:00        1.0   16.478544  17.0        1.0\n",
       "\n",
       "[1102 rows x 6 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predict.sort_values('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature = all_feature[:-58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.481262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123.719040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.245575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.785278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15.125546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-16.117435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.071325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-15.810401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.103111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1044 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predict\n",
       "0   123.481262\n",
       "1   123.719040\n",
       "2   117.245575\n",
       "3   104.785278\n",
       "4    74.003563\n",
       "..         ...\n",
       "53   15.125546\n",
       "54  -16.117435\n",
       "55    1.071325\n",
       "56  -15.810401\n",
       "57    6.103111\n",
       "\n",
       "[1044 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ = []\n",
    "for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "    now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "    time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "all_feature['datetime'] =  np.hstack(time_)\n",
    "all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "all_feature.to_excel(r'feature2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>datetime</th>\n",
       "      <th>offered_x</th>\n",
       "      <th>diff</th>\n",
       "      <th>hour</th>\n",
       "      <th>offered_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.317352</td>\n",
       "      <td>2024-03-19 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.205730</td>\n",
       "      <td>2024-03-20 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.433464</td>\n",
       "      <td>2024-03-21 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.991905</td>\n",
       "      <td>2024-03-22 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.519444</td>\n",
       "      <td>2024-03-25 17:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>151.073898</td>\n",
       "      <td>2024-09-19 11:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.073898</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>152.045944</td>\n",
       "      <td>2024-09-19 13:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.045944</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>130.966293</td>\n",
       "      <td>2024-09-19 16:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.966293</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>85.822189</td>\n",
       "      <td>2024-09-19 16:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.822189</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>17.478544</td>\n",
       "      <td>2024-09-19 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.478544</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict            datetime  offered_x        diff  hour  offered_y\n",
       "0      55.317352 2024-03-19 17:30:00        NaN         NaN   NaN       52.0\n",
       "1      57.205730 2024-03-20 17:30:00        NaN         NaN   NaN       50.0\n",
       "2      57.433464 2024-03-21 17:30:00        NaN         NaN   NaN       75.0\n",
       "3      56.991905 2024-03-22 17:30:00        NaN         NaN   NaN       36.0\n",
       "4      56.519444 2024-03-25 17:30:00        NaN         NaN   NaN       81.0\n",
       "...          ...                 ...        ...         ...   ...        ...\n",
       "1097  151.073898 2024-09-19 11:00:00        1.0  150.073898  11.0        1.0\n",
       "1098  152.045944 2024-09-19 13:00:00        2.0  150.045944  13.0        2.0\n",
       "1099  130.966293 2024-09-19 16:00:00        2.0  128.966293  16.0        2.0\n",
       "1100   85.822189 2024-09-19 16:30:00        2.0   83.822189  16.0        2.0\n",
       "1101   17.478544 2024-09-19 17:00:00        1.0   16.478544  17.0        1.0\n",
       "\n",
       "[1102 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.481262</td>\n",
       "      <td>2024-09-17 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123.719040</td>\n",
       "      <td>2024-09-18 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.245575</td>\n",
       "      <td>2024-09-19 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.785278</td>\n",
       "      <td>2024-09-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.003563</td>\n",
       "      <td>2024-09-23 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15.125546</td>\n",
       "      <td>2024-12-04 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-12-05 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.071325</td>\n",
       "      <td>2024-12-06 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-12-09 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6.103111</td>\n",
       "      <td>2024-12-10 17:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1044 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predict            datetime\n",
       "0   123.481262 2024-09-17 09:00:00\n",
       "1   123.719040 2024-09-18 09:00:00\n",
       "2   117.245575 2024-09-19 09:00:00\n",
       "3   104.785278 2024-09-20 09:00:00\n",
       "4    74.003563 2024-09-23 09:00:00\n",
       "..         ...                 ...\n",
       "53   15.125546 2024-12-04 17:30:00\n",
       "54    0.000000 2024-12-05 17:30:00\n",
       "55    1.071325 2024-12-06 17:30:00\n",
       "56    0.000000 2024-12-09 17:30:00\n",
       "57    6.103111 2024-12-10 17:30:00\n",
       "\n",
       "[1044 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'offered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2871/1282740912.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_mape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'offered'"
     ]
    }
   ],
   "source": [
    "calculate_mape(all_predict.predict, all_predict.offered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean((all_predict.predict-all_predict.offered)**2))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict['hour'] = all_predict.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'offered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2871/1668834394.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'hour is {hour} the mape ：'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcalculate_mape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'offered'"
     ]
    }
   ],
   "source": [
    "for hour in np.unique(all_predict.hour):\n",
    "    _ = all_predict[all_predict.hour == hour]\n",
    "    print(f'hour is {hour} the mape ：',calculate_mape(_.predict, _.offered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hour in np.unique(all_predict.hour):\n",
    "    _ = all_predict[all_predict.hour == hour]\n",
    "    print(f'hour is {hour} the mse ：',(np.mean((_.predict-_.offered)**2))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = all_predict[all_predict.hour == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mape(_.predict, _.offered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_[['predict','datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict.to_excel(r'return.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = lstm_data[lstm_data.hour == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11.groupby('datetime').min().offered.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建示例数据\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_predict['datetime'],  # 将 '日期' 列作为 X 轴\n",
    "    y=all_predict['offered'],  # 将 '销售额' 列作为 Y 轴\n",
    "    mode='lines+markers',  # 折线 + 数据点\n",
    "    name='真实值'  # 图例名称\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_predict['datetime'],  # 将 '日期' 列作为 X 轴\n",
    "    y=all_predict['predict'],  # 将 '销售额' 列作为 Y 轴\n",
    "    mode='lines+markers',  # 折线 + 数据点\n",
    "    name='预测值'  # 图例名称\n",
    "))\n",
    "\n",
    "# 设置坐标轴名称\n",
    "fig.update_layout(\n",
    "    title='折线图示例',\n",
    "    xaxis_title='X轴',\n",
    "    yaxis_title='Y轴',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 可视化训练数据的预测结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot( all_predict['datetime'], all_predict['offered'] , label='Actual Data')\n",
    "plt.plot( all_predict['datetime'], all_predict['predict'], label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 预测未来数据\n",
    "future_steps = 30\n",
    "future_predictions = []\n",
    "\n",
    "last_sequence = X_train[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data.offered.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lstm_data.groupby(['year','month','day']).offered.sum()\n",
    "_.sort_values()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data[X_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 可视化训练数据的预测结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(now_lstm_data[['offered']][1000:].to_numpy(), label='Actual Data')\n",
    "plt.plot(return_y_pred, label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 预测未来数据\n",
    "future_steps = 30\n",
    "future_predictions = []\n",
    "\n",
    "last_sequence = X_train[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y =  model.predict( now_lstm_data[X_columns][:1000].to_numpy().reshape(-1, X_train.shape[1], 1))\n",
    "predict_y = scaler.inverse_transform(np.array(predict_y).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mape(now_lstm_data[['offered']][:1000].to_numpy(), predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ((np.abs((now_lstm_data[['offered']][1000:].to_numpy() - return_y_pred)/now_lstm_data[['offered']][1000:].to_numpy())))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data[['offered']][1000:].to_numpy()[298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "your_venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
