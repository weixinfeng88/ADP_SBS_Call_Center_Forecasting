{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_272967/127859848.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
      "/tmp/ipykernel_272967/127859848.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['date'] = data.datetime.dt.date\n",
      "/tmp/ipykernel_272967/127859848.py:76: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_272967/127859848.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_272967/127859848.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_272967/127859848.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_272967/127859848.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['time'] = data.datetime.dt.time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - loss: 0.0737\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - loss: 0.0557\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.0440\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - loss: 0.0484\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208ms/step - loss: 0.0319\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208ms/step - loss: 0.0510\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.0427\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - loss: 0.0421\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - loss: 0.0582\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - loss: 0.0372\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - loss: 0.0313\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.0404\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - loss: 0.0345\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step - loss: 0.0363\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - loss: 0.0383\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.0349\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - loss: 0.0351\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223ms/step - loss: 0.0333\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - loss: 0.0330\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - loss: 0.0324\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step\n",
      "39.39292791865982\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 205ms/step - loss: 0.0820\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step - loss: 0.0424\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - loss: 0.0616\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - loss: 0.0487\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - loss: 0.0429\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - loss: 0.0370\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.0399\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.0403\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - loss: 0.0335\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.0403\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.0338\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - loss: 0.0392\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - loss: 0.0322\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214ms/step - loss: 0.0343\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214ms/step - loss: 0.0402\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - loss: 0.0359\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - loss: 0.0356\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.0300\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - loss: 0.0313\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step - loss: 0.0357\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
      "37.88060043694501\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "63.142352117889224\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - loss: 0.1107\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0614\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0602\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0598\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - loss: 0.0565\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 0.0576\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0493\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 0.0514\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0547\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.0521\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.0530\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0531\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0480\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0422\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 0.0401\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0563\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0456\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0413\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0512\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0458\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "23.827280065403958\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - loss: 0.0750\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.0790\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0702\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.0600\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0739\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 0.0577\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0542\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 0.0579\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 0.0572\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 0.0500\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - loss: 0.0475\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 0.0507\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 0.0603\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0626\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.0506\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 0.0591\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0643\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0446\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 0.0527\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0543\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "26.483179099400605\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "27.295256585910405\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.1872\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 0.1957\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.1786\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.1631\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 0.1679\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.1689\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.1598\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.1513\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - loss: 0.1497\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 0.1505\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 0.1473\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.1407\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.1357\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.1345\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.1337\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 0.1298\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.1241\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.1202\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - loss: 0.1183\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 0.1157\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "21.57439599927408\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.2155\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - loss: 0.1745\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.1840\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.1860\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.1746\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.1636\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.1590\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.1594\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.1605\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.1575\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.1508\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.1451\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.1423\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.1415\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.1399\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.1360\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.1308\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 0.1273\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - loss: 0.1255\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 0.1215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "20.68872076754165\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "28.480832535645057\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.1266\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0034\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 0.0347\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 0.0566\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 0.0331\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - loss: 0.0076\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 3.7605e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 0.0080\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - loss: 0.0176\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 0.0202\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0152\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 0.0073\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0015\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - loss: 3.1902e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.0030\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - loss: 0.0068\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 0.0088\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.0079\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.0050\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 0.0019\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "1.1733147704234907\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.3001\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0360\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 0.0130\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0697\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 0.0799\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.0516\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.0187\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 0.0015\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.0038\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.0167\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 0.0269\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 0.0275\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 0.0200\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0099\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0025\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 2.0739e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.0024\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 0.0065\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 0.0099\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 0.0108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "8.627887247583155\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "14.742427927324139\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.5484\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 0.0982\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - loss: 0.0023\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 0.0860\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.1480\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 0.1366\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0885\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0403\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 2.0604e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - loss: 0.0055\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0180\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0298\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 0.0366\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0367\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0314\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 0.0228\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - loss: 0.0137\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0062\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - loss: 0.0015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "0.048195690964124925\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.1355\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0233\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 0.0014\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0277\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0421\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 0.0346\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0191\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 0.0063\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 4.6268e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 8.4565e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0046\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0087\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 0.0109\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0107\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0085\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0053\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0024\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - loss: 5.4373e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - loss: 5.2425e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 6.3470e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "4.536913417420796\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "8.319564060918216\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 332\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# 归一化数据\u001b[39;00m\n\u001b[1;32m    331\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 332\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnow_lstm_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# 设置时间步长\u001b[39;00m\n\u001b[1;32m    334\u001b[0m time_step \u001b[38;5;241m=\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    计算 MAPE（平均绝对百分比误差）\n",
    "\n",
    "    参数:\n",
    "    \n",
    "    \n",
    "    actual (array-like): 实际值数组。\n",
    "    predicted (array-like): 预测值数组。\n",
    "\n",
    "    返回:\n",
    "    float: MAPE 值。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 numpy 数组\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 避免除以零的情况\n",
    "    if np.any(actual == 0):\n",
    "        raise ValueError(\"实际值中包含零，无法计算 MAPE。\")\n",
    "\n",
    "    # 计算 MAPE\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mape\n",
    "\n",
    "# set up the interval need to forecast here\n",
    "unique_times = [ datetime.time(9, 0), datetime.time(9, 30), datetime.time(10, 0),\n",
    "datetime.time(10, 30), datetime.time(11, 0), datetime.time(11, 30),\n",
    "datetime.time(12, 0), datetime.time(12, 30), datetime.time(13, 0),\n",
    "datetime.time(13, 30), datetime.time(14, 0), datetime.time(14, 30),\n",
    "datetime.time(15, 0), datetime.time(15, 30), datetime.time(16, 0),\n",
    "datetime.time(16, 30), datetime.time(17, 0), datetime.time(17, 30)\n",
    "]\n",
    "\n",
    "#for month_number in ['2024-11-15']:\n",
    "for month_number in ['2024-11-15']:\n",
    "    #ETL Process\n",
    "    data = pd.read_excel('CS South WFM STAT 2025_05_14.xlsx')\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "           'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "           'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "    \n",
    "    data = data[~data .offered.isna()]\n",
    "    \n",
    "    data = data[data .offered != 0]\n",
    "    # Month one and half need to set up \n",
    "    data_=data\n",
    "    data = data[data.conversation_start_interval_tmst<=pd.to_datetime('{} 00:00:00'.format(month_number))]\n",
    "    holiday_data = pd.read_excel(r'holiday.xlsx')\n",
    "    holiday_data['~is_holiday'] = 0\n",
    "    Holiday_name = ['Christmas Day', 'Columbus Day',\n",
    "           'Independence Day', 'Labor Day', 'Martin Luther King Jr. Day',\n",
    "           'Memorial Day', \"New Year's Day\", \"Presidents' Day\", 'Thanksgiving Day',\n",
    "           'Veterans Day']\n",
    "    holiday_data['date'] = holiday_data.Date.dt.date\n",
    "    data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
    "    data['date'] = data.datetime.dt.date\n",
    "    \n",
    "    data.datetime = data.datetime.dt.floor('T')\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('30T') if x.minute == 29 else x\n",
    "    )\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('H') if x.minute == 59 else x\n",
    "    )\n",
    "    data['time'] = data.datetime.dt.time\n",
    "    data = data.sort_values('datetime').reset_index(drop = True)\n",
    "    data = data.sort_values(by='datetime')\n",
    "    \n",
    "    # 获取所有唯一的时间点（如每天的09:00, 09:30等）\n",
    "    unique_times = data['time'].unique()\n",
    "    \n",
    "    # 获取所有唯一的日期\n",
    "    unique_dates = data['date'].unique()\n",
    "    data = pd.merge(data,holiday_data,on = 'date',how = 'left').fillna(1)\n",
    "    data = data[data['~is_holiday'] == 1]\n",
    "    \n",
    "    holiday_datetime = holiday_data.date.to_numpy()\n",
    "    \n",
    "    start_time = 35+23 # 58 how many days in advance\n",
    "    end_time = start_time+36 #36 is the train length\n",
    "\n",
    "    \n",
    "    add_column = ['actans','actabn','absActHt','absActSa']\n",
    "    \n",
    "    add_columns =  [f'actans_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'actabn_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActHt_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActSa_{i}' for i in range(start_time+1,end_time+1) ]\n",
    "    \n",
    "    matrix = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()\n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered']    \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']    \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])\n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix)\n",
    "    \n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix[i] = np.vstack([matrix[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix = np.vstack(matrix)\n",
    "    \n",
    "    matrix = pd.DataFrame(matrix,columns = ['datetime','offered'] +add_columns +  [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + \\\n",
    "                          [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix['year'] = matrix.datetime.map(lambda x:x.year)\n",
    "    matrix['month'] = matrix.datetime.map(lambda x:x.month)\n",
    "    matrix['day'] = matrix.datetime.map(lambda x:x.day)\n",
    "    matrix['hour'] = matrix.datetime.map(lambda x:x.hour)\n",
    "    matrix['minute'] = matrix.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix = matrix.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data = matrix.iloc[:]\n",
    "    read_data = read_data[~read_data.offered.isna()].reset_index(drop = True)\n",
    "    \n",
    "    fill_na = read_data.iloc[:,1:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na = fill_na.interpolate(method='linear')\n",
    "    \n",
    "    fill_na['datetime'] = read_data.datetime\n",
    "    fill_na['date'] = fill_na.datetime.dt.date\n",
    "    fill_na = fill_na.dropna().reset_index(drop = True)\n",
    "    \n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['year'],prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na['week_of_year'] = fill_na['datetime'].dt.isocalendar().week\n",
    "    fill_na['weekday'] = fill_na['datetime'].dt.day_name()\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['weekday'],prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    matrix_predict = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max()+ pd.offsets.BDay(start_time), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()    \n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered'] \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']  \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])[-start_time:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix_predict.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix_predict)\n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix_predict)):\n",
    "        if matrix_predict[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix_predict[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix_predict[i] = np.vstack([matrix_predict[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix_predict = np.vstack(matrix_predict)\n",
    "    \n",
    "    matrix_predict = pd.DataFrame(matrix_predict,columns = ['datetime','offered'] + add_columns+ [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix_predict['year'] = matrix_predict.datetime.map(lambda x:x.year)\n",
    "    matrix_predict['month'] = matrix_predict.datetime.map(lambda x:x.month)\n",
    "    matrix_predict['day'] = matrix_predict.datetime.map(lambda x:x.day)\n",
    "    matrix_predict['hour'] = matrix_predict.datetime.map(lambda x:x.hour)\n",
    "    matrix_predict['minute'] = matrix_predict.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix_predict = matrix_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data_predict = matrix_predict.iloc[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na_predict = read_data_predict.iloc[:,2:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    fill_na_predict = fill_na_predict.interpolate(method='linear')\n",
    "    fill_na_predict = fill_na_predict.fillna(1)\n",
    "    \n",
    "    fill_na_predict['datetime'] = read_data_predict.datetime\n",
    "    fill_na_predict['date'] = fill_na_predict.datetime.dt.date\n",
    "    \n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['year'], prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na_predict['week_of_year'] = fill_na_predict['datetime'].dt.isocalendar().week\n",
    "    fill_na_predict['weekday'] = fill_na_predict['datetime'].dt.day_name()\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['weekday'], prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    fill_na_predict\n",
    "    \n",
    "    X_columns = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "                \n",
    "    X_columns2 = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "    \n",
    "    \n",
    "    lstm_data = fill_na.dropna(axis = 0)\n",
    "    \n",
    "    lstm_data\n",
    "    \n",
    "    for i in X_columns:\n",
    "        try:\n",
    "            fill_na_predict[i]\n",
    "        except:\n",
    "            fill_na_predict[i] = 0\n",
    "    \n",
    "    lstm_data = lstm_data[lstm_data.year>=2021].reset_index(drop = True)\n",
    "    \n",
    "    week_dict = {'Friday':5, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4}\n",
    "    \n",
    "    lstm_data['week_num'] = lstm_data['weekday'].map(lambda x:week_dict[x])\n",
    "    fill_na_predict['week_num'] = fill_na_predict['weekday'].map(lambda x:week_dict[x])\n",
    "    all_feature = pd.DataFrame()\n",
    "    all_predict=pd.DataFrame()\n",
    "    #for week_ in [1,2,3,4,5]:\n",
    "    for (hour,minute) in lstm_data[['hour','minute']].drop_duplicates().sort_values(['hour','minute']).to_numpy()[:]:\n",
    "        now_lstm_data = lstm_data[(lstm_data.hour == hour)&(lstm_data.minute == minute)]\n",
    "        #now_lstm_data = now_lstm_data[now_lstm_data.week_num == week_]\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        #now_fill_na_predict = now_fill_na_predict[now_fill_na_predict.week_num == week_]\n",
    "    \n",
    "        now_time_list = now_lstm_data.datetime.to_numpy()\n",
    "        now_lstm_data = now_lstm_data[['offered']+['datetime'] +X_columns].reset_index(drop = True)\n",
    "        # 归一化数据\n",
    "    \n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "        X_train_1, y_train =now_lstm_data[X_columns][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_1, y_test =now_lstm_data[X_columns][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_1 = np.array(X_train_1)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_1 = np.array(X_test_1)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_1 = Sequential()\n",
    "        model_1.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_1.add(LSTM(50, return_sequences=False))\n",
    "        model_1.add(Dense(25))\n",
    "        model_1.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_1.fit(X_train_1, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_1 = model_1.predict(X_train_1)\n",
    "        train_predict_1 = scaler.inverse_transform(train_predict_1)\n",
    "        True_data_1 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_1, True_data_1))\n",
    "        return_y_1 =  model_1.predict(X_test_1)\n",
    "        return_y_pred_1 = scaler.inverse_transform(np.array(return_y_1).reshape(-1, 1))\n",
    "        now_predict_1 = pd.DataFrame(return_y_pred_1,columns = ['predict'])\n",
    "        now_predict_1['datetime'] = now_time_list[-start_time:]\n",
    "        now_lstm_data = now_lstm_data[['offered'] +X_columns2].reset_index(drop = True)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "    \n",
    "        X_train_2, y_train =now_lstm_data[X_columns2][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_2, y_test =now_lstm_data[X_columns2][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_2 = np.array(X_train_2)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_2 = np.array(X_test_2)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], X_test_2.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], X_train_2.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_2 = Sequential()\n",
    "        model_2.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_2.add(LSTM(50, return_sequences=False))\n",
    "        model_2.add(Dense(25))\n",
    "        model_2.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_2.fit(X_train_2, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_2 = model_2.predict(X_train_2)\n",
    "        train_predict_2 = scaler.inverse_transform(train_predict_2)\n",
    "        True_data_2 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_2, True_data_2))\n",
    "        return_y_2 =  model_2.predict(X_test_2)\n",
    "        return_y_pred_2 = scaler.inverse_transform(np.array(return_y_2).reshape(-1, 1))\n",
    "        now_predict_2 = pd.DataFrame(return_y_pred_2,columns = ['predict'])\n",
    "        now_predict_2['datetime'] = now_time_list[-start_time:]\n",
    "        print(calculate_mape(y_test, return_y_pred_2))\n",
    "        if calculate_mape(y_test, return_y_pred_1) < calculate_mape(y_test, return_y_pred_2):\n",
    "            all_predict = pd.concat([now_predict_1,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_1.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        else:\n",
    "            all_predict = pd.concat([now_predict_2,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns2].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_2.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        #print('Interval Hour:{}, Minute:{} has been trained'.format(hour,minute)\n",
    "    all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    #all_feature.to_excel(r'return_month_{}.xlsx'.format(month_number))\n",
    "    all_predict['diff'] = (all_predict.predict - all_predict.offered)\n",
    "    all_predict['hour'] = all_predict.datetime.dt.hour\n",
    "    pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    calculate_mape(all_predict.offered,all_predict.predict)\n",
    "    all_predict.to_excel(r'Test_{}.xlsx'.format(month_number))\n",
    "    time_ = []\n",
    "    for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "    all_feature['datetime'] =  np.hstack(time_)\n",
    "    all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "    data_['conversation_start_interval_tmst']=pd.to_datetime(data_['conversation_start_interval_tmst'])\n",
    "    all_feature['datetime']=pd.to_datetime(all_feature['datetime'])\n",
    "    all_feature = pd.merge(all_feature,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n",
    "    all_feature = all_feature.sort_values('datetime').reset_index(drop = True)\n",
    "    all_feature['mape']=np.abs(all_feature['offered']-all_feature['predict'])/(all_feature['offered'])*100\n",
    "    all_feature.to_excel(r'Target_{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered</th>\n",
       "      <th>datetime</th>\n",
       "      <th>freq_59</th>\n",
       "      <th>freq_60</th>\n",
       "      <th>freq_61</th>\n",
       "      <th>freq_62</th>\n",
       "      <th>freq_63</th>\n",
       "      <th>freq_64</th>\n",
       "      <th>freq_65</th>\n",
       "      <th>freq_66</th>\n",
       "      <th>...</th>\n",
       "      <th>absActSa_85</th>\n",
       "      <th>absActSa_86</th>\n",
       "      <th>absActSa_87</th>\n",
       "      <th>absActSa_88</th>\n",
       "      <th>absActSa_89</th>\n",
       "      <th>absActSa_90</th>\n",
       "      <th>absActSa_91</th>\n",
       "      <th>absActSa_92</th>\n",
       "      <th>absActSa_93</th>\n",
       "      <th>absActSa_94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-01-23 07:00:00</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>2.00</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>44738.383394</td>\n",
       "      <td>8310.552381</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>17025.000000</td>\n",
       "      <td>2624.266667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-10-04 07:00:00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>25.00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>...</td>\n",
       "      <td>10485.333333</td>\n",
       "      <td>271.555556</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40230.712121</td>\n",
       "      <td>185.270833</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51051.282353</td>\n",
       "      <td>570.901357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-10-28 07:00:00</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.25</td>\n",
       "      <td>...</td>\n",
       "      <td>15113.158333</td>\n",
       "      <td>215.691667</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20154.841667</td>\n",
       "      <td>192.650000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41623.854167</td>\n",
       "      <td>317.355921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   offered            datetime    freq_59    freq_60  freq_61    freq_62  \\\n",
       "0      1.0 2023-01-23 07:00:00   4.333333   5.333333     2.00  28.333333   \n",
       "1      1.0 2024-10-04 07:00:00  23.000000  27.500000    25.00  13.000000   \n",
       "2      1.0 2024-10-28 07:00:00  10.250000  11.250000    17.25  15.500000   \n",
       "\n",
       "   freq_63    freq_64  freq_65  freq_66  ...   absActSa_85  absActSa_86  \\\n",
       "0      2.8   2.166667      1.2     1.00  ...  44738.383394  8310.552381   \n",
       "1     26.0  19.500000     10.0    19.00  ...  10485.333333   271.555556   \n",
       "2     23.0  12.750000     10.5    12.25  ...  15113.158333   215.691667   \n",
       "\n",
       "   absActSa_87  absActSa_88   absActSa_89  absActSa_90  absActSa_91  \\\n",
       "0    22.000000     1.666667  17025.000000  2624.266667          1.0   \n",
       "1    13.666667     0.000000  40230.712121   185.270833         22.5   \n",
       "2     8.500000     0.000000  20154.841667   192.650000         13.0   \n",
       "\n",
       "   absActSa_92   absActSa_93  absActSa_94  \n",
       "0          0.2      0.000000     1.600000  \n",
       "1          0.0  51051.282353   570.901357  \n",
       "2          0.0  41623.854167   317.355921  \n",
       "\n",
       "[3 rows x 322 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered</th>\n",
       "      <th>actans_59</th>\n",
       "      <th>actans_60</th>\n",
       "      <th>actans_61</th>\n",
       "      <th>actans_62</th>\n",
       "      <th>actans_63</th>\n",
       "      <th>actans_64</th>\n",
       "      <th>actans_65</th>\n",
       "      <th>actans_66</th>\n",
       "      <th>actans_67</th>\n",
       "      <th>...</th>\n",
       "      <th>week_of_year_49</th>\n",
       "      <th>week_of_year_50</th>\n",
       "      <th>week_of_year_51</th>\n",
       "      <th>week_of_year_52</th>\n",
       "      <th>weekday_Friday</th>\n",
       "      <th>weekday_Monday</th>\n",
       "      <th>weekday_Thursday</th>\n",
       "      <th>weekday_Tuesday</th>\n",
       "      <th>weekday_Wednesday</th>\n",
       "      <th>week_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2725.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13099.500000</td>\n",
       "      <td>3399.300000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5437.500000</td>\n",
       "      <td>1836.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41430.000000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17918.000000</td>\n",
       "      <td>103.809524</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48023.772727</td>\n",
       "      <td>6383.333333</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35699.513889</td>\n",
       "      <td>4818.918182</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16681</th>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7203.256000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6953.976000</td>\n",
       "      <td>708.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16682</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7203.256000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16683</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7203.256000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16684</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7203.256000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16685</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7203.256000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16686 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       offered  actans_59  actans_60     actans_61    actans_62  actans_63  \\\n",
       "0          1.0        2.0        0.0    743.000000    29.000000        1.0   \n",
       "1          2.0        5.0        0.0      0.000000  2725.000000        3.0   \n",
       "2          8.0        9.0        0.0  13099.500000  3399.300000        5.0   \n",
       "3          9.0       12.0        0.0  41430.000000  1380.000000       10.0   \n",
       "4         24.0       26.0        2.0  48023.772727  6383.333333       23.0   \n",
       "...        ...        ...        ...           ...          ...        ...   \n",
       "16681     18.0       13.0        1.0   7203.256000   328.000000       14.0   \n",
       "16682      2.0       13.0        1.0   7203.256000   328.000000        0.0   \n",
       "16683      1.0       13.0        1.0   7203.256000   328.000000        0.0   \n",
       "16684      1.0       13.0        1.0   7203.256000   328.000000        0.0   \n",
       "16685      1.0       13.0        1.0   7203.256000   328.000000        0.0   \n",
       "\n",
       "       actans_64     actans_65    actans_66  actans_67  ...  week_of_year_49  \\\n",
       "0            0.0    241.000000     7.000000        2.0  ...                0   \n",
       "1            0.0      0.000000    39.000000        2.0  ...                0   \n",
       "2            1.0   5437.500000  1836.666667        2.0  ...                0   \n",
       "3            0.0  17918.000000   103.809524        9.0  ...                0   \n",
       "4            2.0  35699.513889  4818.918182       26.0  ...                0   \n",
       "...          ...           ...          ...        ...  ...              ...   \n",
       "16681        2.0   6953.976000   708.000000        9.0  ...                0   \n",
       "16682        1.0      1.000000     1.000000        0.0  ...                0   \n",
       "16683        1.0      1.000000     1.000000        0.0  ...                0   \n",
       "16684        1.0      1.000000     1.000000        0.0  ...                0   \n",
       "16685        1.0      1.000000     1.000000        0.0  ...                0   \n",
       "\n",
       "       week_of_year_50  week_of_year_51  week_of_year_52  weekday_Friday  \\\n",
       "0                    0                1                0               0   \n",
       "1                    0                1                0               0   \n",
       "2                    0                1                0               0   \n",
       "3                    0                1                0               0   \n",
       "4                    0                1                0               0   \n",
       "...                ...              ...              ...             ...   \n",
       "16681                0                0                0               0   \n",
       "16682                0                0                0               0   \n",
       "16683                0                0                0               0   \n",
       "16684                0                0                0               0   \n",
       "16685                0                0                0               0   \n",
       "\n",
       "       weekday_Monday  weekday_Thursday  weekday_Tuesday  weekday_Wednesday  \\\n",
       "0                   1                 0                0                  0   \n",
       "1                   1                 0                0                  0   \n",
       "2                   1                 0                0                  0   \n",
       "3                   1                 0                0                  0   \n",
       "4                   1                 0                0                  0   \n",
       "...               ...               ...              ...                ...   \n",
       "16681               0                 1                0                  0   \n",
       "16682               0                 1                0                  0   \n",
       "16683               0                 1                0                  0   \n",
       "16684               0                 1                0                  0   \n",
       "16685               0                 1                0                  0   \n",
       "\n",
       "       week_num  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "...         ...  \n",
       "16681         4  \n",
       "16682         4  \n",
       "16683         4  \n",
       "16684         4  \n",
       "16685         4  \n",
       "\n",
       "[16686 rows x 336 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_272967/512764376.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
      "/tmp/ipykernel_272967/512764376.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['date'] = data.datetime.dt.date\n",
      "/tmp/ipykernel_272967/512764376.py:78: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_272967/512764376.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_272967/512764376.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_272967/512764376.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['datetime'] = data['datetime'].apply(\n",
      "/tmp/ipykernel_272967/512764376.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['time'] = data.datetime.dt.time\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 335\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# 归一化数据\u001b[39;00m\n\u001b[1;32m    334\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 335\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnow_lstm_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# 设置时间步长\u001b[39;00m\n\u001b[1;32m    337\u001b[0m time_step \u001b[38;5;241m=\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "#RS KSR Script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    计算 MAPE（平均绝对百分比误差）\n",
    "\n",
    "    参数:\n",
    "    \n",
    "    \n",
    "    actual (array-like): 实际值数组。\n",
    "    predicted (array-like): 预测值数组。\n",
    "\n",
    "    返回:\n",
    "    float: MAPE 值。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 numpy 数组\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 避免除以零的情况\n",
    "    if np.any(actual == 0):\n",
    "        raise ValueError(\"实际值中包含零，无法计算 MAPE。\")\n",
    "\n",
    "    # 计算 MAPE\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mape\n",
    "\n",
    "unique_times = [ \n",
    "datetime.time(9, 0), datetime.time(9, 30), datetime.time(10, 0),\n",
    "datetime.time(10, 30), datetime.time(11, 0), datetime.time(11, 30),\n",
    "datetime.time(12, 0), datetime.time(12, 30), datetime.time(13, 0),\n",
    "datetime.time(13, 30), datetime.time(14, 0), datetime.time(14, 30),\n",
    "datetime.time(15, 0), datetime.time(15, 30), datetime.time(16, 0),\n",
    "datetime.time(16, 30), datetime.time(17, 0), datetime.time(17, 30),datetime.time(18,0),datetime.time(18,30),datetime.time(19,0)\n",
    "]\n",
    "\n",
    "#for month_number in ['2024-11-15']:\n",
    "for month_number in ['2024-11-15','2024-12-15','2025-1-15','2025-2-15','2025-3-15']:\n",
    "    #ETL Process\n",
    "    data = pd.read_excel('RS_KSR_WFM_STAT_2025_05_14.xlsx')\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "           'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "           'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "    \n",
    "    data = data[~data .offered.isna()]\n",
    "    \n",
    "    data = data[data .offered != 0]\n",
    "    # Month one and half need to set up \n",
    "    data_=data\n",
    "    data = data[data.conversation_start_interval_tmst<=pd.to_datetime('{} 00:00:00'.format(month_number))]\n",
    "    holiday_data = pd.read_excel(r'holiday.xlsx')\n",
    "    holiday_data['~is_holiday'] = 0\n",
    "    Holiday_name = ['Christmas Day', 'Columbus Day',\n",
    "           'Independence Day', 'Labor Day', 'Martin Luther King Jr. Day',\n",
    "           'Memorial Day', \"New Year's Day\", \"Presidents' Day\", 'Thanksgiving Day',\n",
    "           'Veterans Day']\n",
    "    holiday_data['date'] = holiday_data.Date.dt.date\n",
    "    data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
    "    data['date'] = data.datetime.dt.date\n",
    "    \n",
    "    data.datetime = data.datetime.dt.floor('T')\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('30T') if x.minute == 29 else x\n",
    "    )\n",
    "    data['datetime'] = data['datetime'].apply(\n",
    "        lambda x: x.ceil('H') if x.minute == 59 else x\n",
    "    )\n",
    "    data['time'] = data.datetime.dt.time\n",
    "    data = data.sort_values('datetime').reset_index(drop = True)\n",
    "    data = data.sort_values(by='datetime')\n",
    "    \n",
    "    # 获取所有唯一的时间点（如每天的09:00, 09:30等）\n",
    "    unique_times = data['time'].unique()\n",
    "    \n",
    "    # 获取所有唯一的日期\n",
    "    unique_dates = data['date'].unique()\n",
    "    data = pd.merge(data,holiday_data,on = 'date',how = 'left').fillna(1)\n",
    "    data = data[data['~is_holiday'] == 1]\n",
    "    \n",
    "    holiday_datetime = holiday_data.date.to_numpy()\n",
    "    \n",
    "    start_time = 35+23 # 58 how many days in advance\n",
    "    end_time = start_time+36 #36 is the train length\n",
    "    # set up the interval need to forecast here\n",
    "\n",
    "    \n",
    "    add_column = ['actans','actabn','absActHt','absActSa']\n",
    "    \n",
    "    add_columns =  [f'actans_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'actabn_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActHt_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                    [f'absActSa_{i}' for i in range(start_time+1,end_time+1) ]\n",
    "    \n",
    "    matrix = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()\n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered']    \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']    \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])\n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix)\n",
    "    \n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix[i] = np.vstack([matrix[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix = np.vstack(matrix)\n",
    "    \n",
    "    matrix = pd.DataFrame(matrix,columns = ['datetime','offered'] +add_columns +  [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + \\\n",
    "                          [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix['year'] = matrix.datetime.map(lambda x:x.year)\n",
    "    matrix['month'] = matrix.datetime.map(lambda x:x.month)\n",
    "    matrix['day'] = matrix.datetime.map(lambda x:x.day)\n",
    "    matrix['hour'] = matrix.datetime.map(lambda x:x.hour)\n",
    "    matrix['minute'] = matrix.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix = matrix.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data = matrix.iloc[:]\n",
    "    read_data = read_data[~read_data.offered.isna()].reset_index(drop = True)\n",
    "    \n",
    "    fill_na = read_data.iloc[:,1:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na = fill_na.interpolate(method='linear')\n",
    "    \n",
    "    fill_na['datetime'] = read_data.datetime\n",
    "    fill_na['date'] = fill_na.datetime.dt.date\n",
    "    fill_na = fill_na.dropna().reset_index(drop = True)\n",
    "    \n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['year'],prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na['week_of_year'] = fill_na['datetime'].dt.isocalendar().week\n",
    "    fill_na['weekday'] = fill_na['datetime'].dt.day_name()\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na = pd.concat([fill_na,pd.get_dummies(fill_na['weekday'],prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    matrix_predict = []\n",
    "    \n",
    "    # 遍历每个时间点\n",
    "    for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "        # 过滤出当前时间点的数据\n",
    "        time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "        full_dates = pd.date_range(start=unique_dates.min(), end=unique_dates.max()+ pd.offsets.BDay(start_time), freq='B')  # 仅工作日\n",
    "        full_dates = full_dates.difference(holiday_datetime)\n",
    "        time_data = time_data.groupby(time_data.index).sum()    \n",
    "        time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "        add_data = add_data.groupby(add_data.index).sum()\n",
    "        add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        time_data1 = data[data['time'] == time1].set_index('date')['offered'] \n",
    "        time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "        time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        time_data2 = data[data['time'] == time2].set_index('date')['offered']  \n",
    "        time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "        time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "        shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "        \n",
    "        \n",
    "        year_data = time_data.shift(251)\n",
    "        \n",
    "    \n",
    "        # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "        combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "        \n",
    "        # 创建日期+时间列\n",
    "        datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "        \n",
    "        # 将日期+时间列添加到数据中\n",
    "        combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])[-start_time:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 将结果存入矩阵\n",
    "        matrix_predict.append(combined_data_with_datetime)\n",
    "    \n",
    "    max_rows = max(arr.shape[0] for arr in matrix_predict)\n",
    "    # 将每个时间点的数据填充到最大行数\n",
    "    for i in range(len(matrix_predict)):\n",
    "        if matrix_predict[i].shape[0] < max_rows:\n",
    "            padding = np.full((max_rows - matrix_predict[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "            matrix_predict[i] = np.vstack([matrix_predict[i], padding])\n",
    "    \n",
    "    # 将矩阵堆叠成一个大的二维数组\n",
    "    matrix_predict = np.vstack(matrix_predict)\n",
    "    \n",
    "    matrix_predict = pd.DataFrame(matrix_predict,columns = ['datetime','offered'] + add_columns+ [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                          + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                           ['years_data']).sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    matrix_predict['year'] = matrix_predict.datetime.map(lambda x:x.year)\n",
    "    matrix_predict['month'] = matrix_predict.datetime.map(lambda x:x.month)\n",
    "    matrix_predict['day'] = matrix_predict.datetime.map(lambda x:x.day)\n",
    "    matrix_predict['hour'] = matrix_predict.datetime.map(lambda x:x.hour)\n",
    "    matrix_predict['minute'] = matrix_predict.datetime.map(lambda x:x.minute)\n",
    "    \n",
    "    matrix_predict = matrix_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    \n",
    "    read_data_predict = matrix_predict.iloc[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fill_na_predict = read_data_predict.iloc[:,2:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "    fill_na_predict = fill_na_predict.interpolate(method='linear')\n",
    "    fill_na_predict = fill_na_predict.fillna(1)\n",
    "    \n",
    "    fill_na_predict['datetime'] = read_data_predict.datetime\n",
    "    fill_na_predict['date'] = fill_na_predict.datetime.dt.date\n",
    "    \n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['month'],prefix = 'month').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['year'], prefix = 'year').astype(int)],axis = 1)\n",
    "    fill_na_predict['week_of_year'] = fill_na_predict['datetime'].dt.isocalendar().week\n",
    "    fill_na_predict['weekday'] = fill_na_predict['datetime'].dt.day_name()\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['week_of_year'],prefix = 'week_of_year').astype(int)],axis = 1)\n",
    "    fill_na_predict = pd.concat([fill_na_predict,pd.get_dummies(fill_na_predict['weekday'], prefix = 'weekday').astype(int)],axis = 1)\n",
    "    \n",
    "    fill_na_predict\n",
    "    \n",
    "    X_columns = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "                \n",
    "    X_columns2 = [f'freq_{i}' for i in range(start_time+1,end_time+1) ] + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                [f'freq_next{i}' for i in range(start_time+1,end_time+1) ] + ['years_data'] + [f'month_{i}' for i in range(1,13)] +\\\n",
    "                [f'year_{i}' for i in np.unique(fill_na.year)] +\\\n",
    "                [f'week_of_year_{i}' for i in fill_na.week_of_year.drop_duplicates().tolist()] +\\\n",
    "                    add_columns\n",
    "    \n",
    "    \n",
    "    lstm_data = fill_na.dropna(axis = 0)\n",
    "    \n",
    "    lstm_data\n",
    "    \n",
    "    for i in X_columns:\n",
    "        try:\n",
    "            fill_na_predict[i]\n",
    "        except:\n",
    "            fill_na_predict[i] = 0\n",
    "    \n",
    "    lstm_data = lstm_data[lstm_data.year>=2021].reset_index(drop = True)\n",
    "    \n",
    "    week_dict = {'Friday':5, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4}\n",
    "    \n",
    "    lstm_data['week_num'] = lstm_data['weekday'].map(lambda x:week_dict[x])\n",
    "    fill_na_predict['week_num'] = fill_na_predict['weekday'].map(lambda x:week_dict[x])\n",
    "    all_feature = pd.DataFrame()\n",
    "    all_predict=pd.DataFrame()\n",
    "    #for week_ in [1,2,3,4,5]:\n",
    "    for (hour,minute) in lstm_data[['hour','minute']].drop_duplicates().sort_values(['hour','minute']).to_numpy()[:]:\n",
    "        now_lstm_data = lstm_data[(lstm_data.hour == hour)&(lstm_data.minute == minute)]\n",
    "        #now_lstm_data = now_lstm_data[now_lstm_data.week_num == week_]\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        #now_fill_na_predict = now_fill_na_predict[now_fill_na_predict.week_num == week_]\n",
    "    \n",
    "        now_time_list = now_lstm_data.datetime.to_numpy()\n",
    "        now_lstm_data = now_lstm_data[['offered']+['datetime'] +X_columns].reset_index(drop = True)\n",
    "        # 归一化数据\n",
    "    \n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "        X_train_1, y_train =now_lstm_data[X_columns][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_1, y_test =now_lstm_data[X_columns][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_1 = np.array(X_train_1)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_1 = np.array(X_test_1)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_1 = Sequential()\n",
    "        model_1.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_1.add(LSTM(50, return_sequences=False))\n",
    "        model_1.add(Dense(25))\n",
    "        model_1.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_1.fit(X_train_1, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_1 = model_1.predict(X_train_1)\n",
    "        train_predict_1 = scaler.inverse_transform(train_predict_1)\n",
    "        True_data_1 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_1, True_data_1))\n",
    "        return_y_1 =  model_1.predict(X_test_1)\n",
    "        return_y_pred_1 = scaler.inverse_transform(np.array(return_y_1).reshape(-1, 1))\n",
    "        now_predict_1 = pd.DataFrame(return_y_pred_1,columns = ['predict'])\n",
    "        now_predict_1['datetime'] = now_time_list[-start_time:]\n",
    "        now_lstm_data = now_lstm_data[['offered'] +X_columns2].reset_index(drop = True)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(now_lstm_data[['offered']][:-start_time])\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "    \n",
    "        X_train_2, y_train =now_lstm_data[X_columns2][:-start_time] , scaled_data\n",
    "    \n",
    "        X_test_2, y_test =now_lstm_data[X_columns2][-start_time:] , now_lstm_data[['offered']][-start_time:]\n",
    "    \n",
    "        X_train_2 = np.array(X_train_2)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_2 = np.array(X_test_2)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], X_test_2.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], X_train_2.shape[1], 1)\n",
    "        # 构建LSTM模型\n",
    "        model_2 = Sequential()\n",
    "        model_2.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_2.add(LSTM(50, return_sequences=False))\n",
    "        model_2.add(Dense(25))\n",
    "        model_2.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_2.fit(X_train_2, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_2 = model_2.predict(X_train_2)\n",
    "        train_predict_2 = scaler.inverse_transform(train_predict_2)\n",
    "        True_data_2 = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_2, True_data_2))\n",
    "        return_y_2 =  model_2.predict(X_test_2)\n",
    "        return_y_pred_2 = scaler.inverse_transform(np.array(return_y_2).reshape(-1, 1))\n",
    "        now_predict_2 = pd.DataFrame(return_y_pred_2,columns = ['predict'])\n",
    "        now_predict_2['datetime'] = now_time_list[-start_time:]\n",
    "        print(calculate_mape(y_test, return_y_pred_2))\n",
    "        if calculate_mape(y_test, return_y_pred_1) < calculate_mape(y_test, return_y_pred_2):\n",
    "            all_predict = pd.concat([now_predict_1,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_1.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        else:\n",
    "            all_predict = pd.concat([now_predict_2,all_predict])\n",
    "            future_X = now_fill_na_predict[X_columns2].to_numpy()\n",
    "            future_data = scaler.inverse_transform(np.array(\\\n",
    "                            model_2.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        #print('Interval Hour:{}, Minute:{} has been trained'.format(hour,minute)\n",
    "    all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    #all_feature.to_excel(r'return_month_{}.xlsx'.format(month_number))\n",
    "    all_predict['diff'] = (all_predict.predict - all_predict.offered)\n",
    "    all_predict['hour'] = all_predict.datetime.dt.hour\n",
    "    pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    calculate_mape(all_predict.offered,all_predict.predict)\n",
    "    all_predict.to_excel(r'RS_KSR_Test_{}.xlsx'.format(month_number))\n",
    "    time_ = []\n",
    "    for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "    all_feature['datetime'] =  np.hstack(time_)\n",
    "    all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "    data_['conversation_start_interval_tmst']=pd.to_datetime(data_['conversation_start_interval_tmst'])\n",
    "    all_feature['datetime']=pd.to_datetime(all_feature['datetime'])\n",
    "    all_feature = pd.merge(all_feature,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n",
    "    all_feature = all_feature.sort_values('datetime').reset_index(drop = True)\n",
    "    all_feature['mape']=np.abs(all_feature['offered']-all_feature['predict'])/(all_feature['offered'])*100\n",
    "    all_feature.to_excel(r'RS_KSR_Target_{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered</th>\n",
       "      <th>datetime</th>\n",
       "      <th>freq_59</th>\n",
       "      <th>freq_60</th>\n",
       "      <th>freq_61</th>\n",
       "      <th>freq_62</th>\n",
       "      <th>freq_63</th>\n",
       "      <th>freq_64</th>\n",
       "      <th>freq_65</th>\n",
       "      <th>freq_66</th>\n",
       "      <th>...</th>\n",
       "      <th>absActSa_85</th>\n",
       "      <th>absActSa_86</th>\n",
       "      <th>absActSa_87</th>\n",
       "      <th>absActSa_88</th>\n",
       "      <th>absActSa_89</th>\n",
       "      <th>absActSa_90</th>\n",
       "      <th>absActSa_91</th>\n",
       "      <th>absActSa_92</th>\n",
       "      <th>absActSa_93</th>\n",
       "      <th>absActSa_94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-01-23 07:00:00</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>2.00</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>44738.383394</td>\n",
       "      <td>8310.552381</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>17025.000000</td>\n",
       "      <td>2624.266667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-10-04 07:00:00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>25.00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>...</td>\n",
       "      <td>10485.333333</td>\n",
       "      <td>271.555556</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40230.712121</td>\n",
       "      <td>185.270833</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51051.282353</td>\n",
       "      <td>570.901357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-10-28 07:00:00</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>17.25</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.25</td>\n",
       "      <td>...</td>\n",
       "      <td>15113.158333</td>\n",
       "      <td>215.691667</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20154.841667</td>\n",
       "      <td>192.650000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41623.854167</td>\n",
       "      <td>317.355921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   offered            datetime    freq_59    freq_60  freq_61    freq_62  \\\n",
       "0      1.0 2023-01-23 07:00:00   4.333333   5.333333     2.00  28.333333   \n",
       "1      1.0 2024-10-04 07:00:00  23.000000  27.500000    25.00  13.000000   \n",
       "2      1.0 2024-10-28 07:00:00  10.250000  11.250000    17.25  15.500000   \n",
       "\n",
       "   freq_63    freq_64  freq_65  freq_66  ...   absActSa_85  absActSa_86  \\\n",
       "0      2.8   2.166667      1.2     1.00  ...  44738.383394  8310.552381   \n",
       "1     26.0  19.500000     10.0    19.00  ...  10485.333333   271.555556   \n",
       "2     23.0  12.750000     10.5    12.25  ...  15113.158333   215.691667   \n",
       "\n",
       "   absActSa_87  absActSa_88   absActSa_89  absActSa_90  absActSa_91  \\\n",
       "0    22.000000     1.666667  17025.000000  2624.266667          1.0   \n",
       "1    13.666667     0.000000  40230.712121   185.270833         22.5   \n",
       "2     8.500000     0.000000  20154.841667   192.650000         13.0   \n",
       "\n",
       "   absActSa_92   absActSa_93  absActSa_94  \n",
       "0          0.2      0.000000     1.600000  \n",
       "1          0.0  51051.282353   570.901357  \n",
       "2          0.0  41623.854167   317.355921  \n",
       "\n",
       "[3 rows x 322 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n",
    "#all_feature.to_excel(r'return_month_{}.xlsx'.format(month_number))\n",
    "all_predict['diff'] = (all_predict.predict - all_predict.offered)\n",
    "all_predict['hour'] = all_predict.datetime.dt.hour\n",
    "pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "calculate_mape(all_predict.offered,all_predict.predict)\n",
    "all_predict.to_excel(r'Test_{}.xlsx'.format(month_number))\n",
    "time_ = []\n",
    "for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "    now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "    time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "all_feature['datetime'] =  np.hstack(time_)\n",
    "all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "data_['conversation_start_interval_tmst']=pd.to_datetime(data_['conversation_start_interval_tmst'])\n",
    "all_feature['datetime']=pd.to_datetime(all_feature['datetime'])\n",
    "all_feature = pd.merge(all_feature,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n",
    "all_feature = all_feature.sort_values('datetime').reset_index(drop = True)\n",
    "all_feature['mape']=np.abs(all_feature['offered']-all_feature['predict'])/(all_feature['offered'])*100\n",
    "all_feature.to_excel(r'Target_{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ = []\n",
    "for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "    now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "    time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "all_feature['datetime'] =  np.hstack(time_)\n",
    "all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "data_['conversation_start_interval_tmst']=pd.to_datetime(data_['conversation_start_interval_tmst'])\n",
    "all_feature['datetime']=pd.to_datetime(all_feature['datetime'])\n",
    "all_feature = pd.merge(all_feature,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n",
    "all_feature = all_feature.sort_values('datetime').reset_index(drop = True)\n",
    "all_feature['mape']=np.abs(all_feature['offered']-all_feature['predict'])/(all_feature['offered'])*100\n",
    "all_feature.to_excel(r'Target_{}.xlsx'.format(month_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offered</th>\n",
       "      <th>actans_59</th>\n",
       "      <th>actans_60</th>\n",
       "      <th>actans_61</th>\n",
       "      <th>actans_62</th>\n",
       "      <th>actans_63</th>\n",
       "      <th>actans_64</th>\n",
       "      <th>actans_65</th>\n",
       "      <th>actans_66</th>\n",
       "      <th>actans_67</th>\n",
       "      <th>...</th>\n",
       "      <th>week_of_year_49</th>\n",
       "      <th>week_of_year_50</th>\n",
       "      <th>week_of_year_51</th>\n",
       "      <th>week_of_year_52</th>\n",
       "      <th>weekday_Friday</th>\n",
       "      <th>weekday_Monday</th>\n",
       "      <th>weekday_Thursday</th>\n",
       "      <th>weekday_Tuesday</th>\n",
       "      <th>weekday_Wednesday</th>\n",
       "      <th>week_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59345.142857</td>\n",
       "      <td>10578.598291</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42378.666667</td>\n",
       "      <td>18984.573810</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>67703.807692</td>\n",
       "      <td>42008.133333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42739.285714</td>\n",
       "      <td>18532.800000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>79839.384615</td>\n",
       "      <td>67299.619048</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>119072.250000</td>\n",
       "      <td>46178.314286</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>79216.400000</td>\n",
       "      <td>113523.850000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68695.071429</td>\n",
       "      <td>47624.700000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92581.000000</td>\n",
       "      <td>73993.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>137439.120000</td>\n",
       "      <td>34600.866667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10720</th>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39421.079365</td>\n",
       "      <td>12697.911111</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47067.414286</td>\n",
       "      <td>1606.095238</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10721</th>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33429.733333</td>\n",
       "      <td>11964.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80904.500000</td>\n",
       "      <td>27354.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40212.250000</td>\n",
       "      <td>32933.952381</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53200.250000</td>\n",
       "      <td>4747.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10723</th>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>67221.700000</td>\n",
       "      <td>16730.133333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19897.166667</td>\n",
       "      <td>221.466667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10724</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>67221.700000</td>\n",
       "      <td>16730.133333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10890.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10725 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       offered  actans_59  actans_60     actans_61      actans_62  actans_63  \\\n",
       "0         37.0       32.0        1.0  59345.142857   10578.598291       29.0   \n",
       "1         17.0       34.0        3.0  67703.807692   42008.133333       24.0   \n",
       "2         33.0       24.0       17.0  79839.384615   67299.619048       34.0   \n",
       "3         40.0       33.0        9.0  79216.400000  113523.850000       27.0   \n",
       "4         38.0       20.0        6.0  92581.000000   73993.000000       58.0   \n",
       "...        ...        ...        ...           ...            ...        ...   \n",
       "10720     22.0       19.0        1.0  39421.079365   12697.911111       17.0   \n",
       "10721     18.0       11.0        3.0  33429.733333   11964.333333       21.0   \n",
       "10722     12.0       13.0        3.0  40212.250000   32933.952381       21.0   \n",
       "10723      9.0       13.0        3.0  67221.700000   16730.133333       11.0   \n",
       "10724      1.0       13.0        3.0  67221.700000   16730.133333        6.0   \n",
       "\n",
       "       actans_64      actans_65     actans_66  actans_67  ...  \\\n",
       "0            2.0   42378.666667  18984.573810       35.0  ...   \n",
       "1            3.0   42739.285714  18532.800000       24.0  ...   \n",
       "2            3.0  119072.250000  46178.314286       26.0  ...   \n",
       "3            1.0   68695.071429  47624.700000       21.0  ...   \n",
       "4            5.0  137439.120000  34600.866667       32.0  ...   \n",
       "...          ...            ...           ...        ...  ...   \n",
       "10720        0.0   47067.414286   1606.095238       19.0  ...   \n",
       "10721        5.0   80904.500000  27354.000000       31.0  ...   \n",
       "10722        2.0   53200.250000   4747.166667       20.0  ...   \n",
       "10723        0.0   19897.166667    221.466667       15.0  ...   \n",
       "10724        0.0   10890.000000     27.000000       15.0  ...   \n",
       "\n",
       "       week_of_year_49  week_of_year_50  week_of_year_51  week_of_year_52  \\\n",
       "0                    0                0                1                0   \n",
       "1                    0                0                1                0   \n",
       "2                    0                0                1                0   \n",
       "3                    0                0                1                0   \n",
       "4                    0                0                1                0   \n",
       "...                ...              ...              ...              ...   \n",
       "10720                0                0                0                0   \n",
       "10721                0                0                0                0   \n",
       "10722                0                0                0                0   \n",
       "10723                0                0                0                0   \n",
       "10724                0                0                0                0   \n",
       "\n",
       "       weekday_Friday  weekday_Monday  weekday_Thursday  weekday_Tuesday  \\\n",
       "0                   1               0                 0                0   \n",
       "1                   1               0                 0                0   \n",
       "2                   1               0                 0                0   \n",
       "3                   1               0                 0                0   \n",
       "4                   1               0                 0                0   \n",
       "...               ...             ...               ...              ...   \n",
       "10720               0               0                 1                0   \n",
       "10721               0               0                 1                0   \n",
       "10722               0               0                 1                0   \n",
       "10723               0               0                 1                0   \n",
       "10724               0               0                 1                0   \n",
       "\n",
       "       weekday_Wednesday  week_num  \n",
       "0                      0         5  \n",
       "1                      0         5  \n",
       "2                      0         5  \n",
       "3                      0         5  \n",
       "4                      0         5  \n",
       "...                  ...       ...  \n",
       "10720                  0         4  \n",
       "10721                  0         4  \n",
       "10722                  0         4  \n",
       "10723                  0         4  \n",
       "10724                  0         4  \n",
       "\n",
       "[10725 rows x 336 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour is 9.0 the mape ： 15.108565395142312\n",
      "hour is 10.0 the mape ： 14.543283188637835\n",
      "hour is 11.0 the mape ： 14.113005774019918\n",
      "hour is 12.0 the mape ： 12.645508299088789\n",
      "hour is 13.0 the mape ： 12.244234930406705\n",
      "hour is 14.0 the mape ： 13.019868702897055\n",
      "hour is 15.0 the mape ： 13.78376280752372\n",
      "hour is 16.0 the mape ： 19.91938217071393\n",
      "hour is 17.0 the mape ： 23.89058659019716\n",
      "hour is nan the mape ： nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for hour in np.unique(all_predict.hour):\n",
    "    _ = all_predict[all_predict.hour == hour]\n",
    "    print(f'hour is {hour} the mape ：',calculate_mape(_.predict, _.offered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hour in np.unique(all_predict.hour):\n",
    "    _ = all_predict[all_predict.hour == hour]\n",
    "    print(f'hour is {hour} the mse ：',(np.mean((_.predict-_.offered)**2))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = all_predict[all_predict.hour == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mape(_.predict, _.offered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_ = pd.merge(all_feature_,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_=all_feature[all_feature['datetime'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(25.134611845200947)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_feature_[all_feature_['datetime']>=pd.to_datetime('2025-01-20 00:00:00')]['mape']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738     33.208509\n",
       "739      8.285545\n",
       "740     10.427519\n",
       "741     19.570874\n",
       "742     15.077025\n",
       "743     14.471300\n",
       "744     16.976033\n",
       "745      0.516770\n",
       "746     25.660279\n",
       "747     15.891084\n",
       "748     17.562603\n",
       "749     27.768948\n",
       "750     23.178524\n",
       "751      8.534840\n",
       "752      6.079927\n",
       "753     16.742529\n",
       "754     21.701106\n",
       "755      7.490306\n",
       "756     30.779100\n",
       "757     38.321014\n",
       "758     29.270353\n",
       "759     32.218009\n",
       "760     32.037269\n",
       "761     33.295054\n",
       "762     34.275165\n",
       "763     36.718296\n",
       "764     38.162828\n",
       "765     32.324275\n",
       "766     41.023315\n",
       "767     35.946070\n",
       "768     33.119583\n",
       "769     27.651849\n",
       "770     25.984501\n",
       "771     30.806241\n",
       "772     45.436389\n",
       "773     32.127987\n",
       "774     41.158032\n",
       "775     26.985486\n",
       "776     27.486449\n",
       "777     25.744703\n",
       "778     27.339127\n",
       "779     26.948297\n",
       "780     32.965840\n",
       "781     17.338157\n",
       "782     41.068036\n",
       "783     26.684955\n",
       "784     37.725078\n",
       "785     25.027156\n",
       "786     26.288829\n",
       "787     40.379822\n",
       "788     26.710627\n",
       "789     22.060587\n",
       "790     54.258946\n",
       "791     34.754110\n",
       "792     33.948697\n",
       "793     34.455973\n",
       "794     17.733702\n",
       "795     37.769661\n",
       "796     31.449613\n",
       "797     23.135530\n",
       "798     16.617755\n",
       "799     20.287936\n",
       "800     26.369720\n",
       "801     16.205300\n",
       "802     30.060864\n",
       "803     31.745425\n",
       "804     28.728549\n",
       "805     21.987989\n",
       "806     20.670148\n",
       "807     22.733748\n",
       "808     44.851734\n",
       "809     32.360325\n",
       "810     23.821764\n",
       "811     35.920351\n",
       "812     33.386747\n",
       "813     32.324778\n",
       "814     20.986634\n",
       "815     17.123265\n",
       "816     26.606282\n",
       "817     11.431662\n",
       "818     28.635113\n",
       "819     29.664624\n",
       "820     34.543217\n",
       "821     22.959531\n",
       "822     27.847785\n",
       "823      8.059409\n",
       "824     28.549007\n",
       "825     19.590624\n",
       "826     42.686447\n",
       "827      1.836616\n",
       "828     33.855375\n",
       "829     26.971786\n",
       "830      8.893134\n",
       "831     10.313610\n",
       "832     21.620565\n",
       "833      7.665983\n",
       "834     13.118328\n",
       "835     28.433979\n",
       "836     14.582719\n",
       "837     19.916206\n",
       "838     18.078584\n",
       "839     26.140690\n",
       "840     10.226462\n",
       "841     26.100243\n",
       "842     19.836072\n",
       "843      4.631073\n",
       "844     31.838957\n",
       "845     13.613151\n",
       "846     25.513627\n",
       "847     16.762948\n",
       "848     11.054224\n",
       "849     14.342127\n",
       "850     19.784927\n",
       "851     17.730110\n",
       "852     12.552372\n",
       "853     20.094365\n",
       "854     38.453523\n",
       "855     24.605724\n",
       "856     10.026995\n",
       "857     29.744611\n",
       "858      3.135419\n",
       "859      8.034323\n",
       "860     13.624462\n",
       "861     11.328365\n",
       "862     36.994911\n",
       "863      8.671729\n",
       "864     21.106271\n",
       "865     13.253684\n",
       "866      2.404909\n",
       "867     13.486548\n",
       "868     13.764699\n",
       "869     18.388490\n",
       "870     28.077455\n",
       "871     16.324234\n",
       "872     25.482204\n",
       "873     11.877486\n",
       "874     25.869894\n",
       "875     31.937135\n",
       "876      9.882733\n",
       "877     23.464170\n",
       "878      8.262758\n",
       "879     12.431101\n",
       "880     23.709495\n",
       "881     18.390942\n",
       "882     39.812013\n",
       "883     29.401852\n",
       "884      0.392137\n",
       "885     26.758312\n",
       "886     25.043036\n",
       "887     37.324623\n",
       "888     31.582032\n",
       "889     34.874967\n",
       "890     34.395258\n",
       "891     25.334993\n",
       "892     42.195979\n",
       "893     30.727838\n",
       "894     15.716484\n",
       "895     19.442776\n",
       "896     13.693665\n",
       "897      9.189172\n",
       "898     45.575477\n",
       "899     32.448190\n",
       "900     27.764726\n",
       "901     23.903369\n",
       "902     12.942869\n",
       "903     31.053246\n",
       "904     21.380723\n",
       "905     21.480973\n",
       "906     27.572591\n",
       "907     20.814738\n",
       "908     21.843341\n",
       "909     18.173390\n",
       "910     34.536425\n",
       "911     11.598656\n",
       "912     28.382175\n",
       "913     20.768041\n",
       "914      3.243872\n",
       "915      5.426679\n",
       "916     13.815605\n",
       "917      6.177852\n",
       "918     41.490423\n",
       "919     22.153517\n",
       "920     17.015872\n",
       "921     35.294070\n",
       "922     28.229066\n",
       "923     25.679790\n",
       "924     29.577721\n",
       "925     36.960586\n",
       "926     27.410707\n",
       "927     30.365219\n",
       "928     39.803325\n",
       "929     37.867922\n",
       "930     33.446135\n",
       "931     21.734013\n",
       "932     25.939234\n",
       "933     35.869489\n",
       "934     45.804309\n",
       "935     43.615352\n",
       "936     36.562719\n",
       "937     23.719249\n",
       "938     17.074625\n",
       "939     38.320730\n",
       "940     29.371560\n",
       "941     26.279611\n",
       "942     27.055606\n",
       "943     31.469502\n",
       "944     36.245384\n",
       "945     29.974651\n",
       "946     36.284762\n",
       "947     28.832269\n",
       "948     26.645803\n",
       "949     19.644833\n",
       "950     22.348391\n",
       "951     12.396618\n",
       "952     31.990609\n",
       "953     15.132473\n",
       "954     51.919958\n",
       "955     34.122941\n",
       "956     35.586272\n",
       "957     35.248972\n",
       "958     24.218349\n",
       "959     23.197120\n",
       "960     29.869408\n",
       "961     30.325722\n",
       "962     37.330201\n",
       "963     36.173968\n",
       "964     32.419438\n",
       "965     32.836007\n",
       "966     30.213923\n",
       "967     33.326309\n",
       "968     18.315218\n",
       "969     13.885177\n",
       "970     57.020151\n",
       "971     35.466516\n",
       "972     43.634102\n",
       "973     27.684068\n",
       "974     20.912901\n",
       "975     26.376404\n",
       "976     33.491108\n",
       "977     23.293321\n",
       "978     30.420549\n",
       "979     26.321549\n",
       "980     31.163154\n",
       "981     31.822707\n",
       "982     30.923742\n",
       "983     38.730498\n",
       "984     28.627196\n",
       "985     18.427794\n",
       "986     31.318848\n",
       "987     35.842332\n",
       "988     52.071967\n",
       "989     52.538351\n",
       "990     48.601880\n",
       "991     28.582261\n",
       "992     30.014019\n",
       "993     33.642194\n",
       "994     35.193202\n",
       "995     21.102170\n",
       "996     28.679913\n",
       "997     25.215521\n",
       "998     40.293029\n",
       "999     36.727598\n",
       "1000    37.562268\n",
       "1001    22.086025\n",
       "1002    29.502776\n",
       "1003    22.760895\n",
       "1004    11.683735\n",
       "1005    11.266413\n",
       "1006    37.848121\n",
       "1007    33.937211\n",
       "1008    36.807181\n",
       "1009     3.063950\n",
       "1010    17.440113\n",
       "1011    29.763439\n",
       "1012    33.423667\n",
       "1013    14.012622\n",
       "1014    15.559052\n",
       "1015    24.475651\n",
       "1016    31.009130\n",
       "1017    29.504531\n",
       "1018    35.102208\n",
       "1019    29.253646\n",
       "1020     3.168758\n",
       "1021    14.871893\n",
       "1022    15.247325\n",
       "1023    20.342340\n",
       "1024    35.236714\n",
       "1025    15.036240\n",
       "1026    19.202910\n",
       "1027    15.472636\n",
       "1028    11.589154\n",
       "1029    11.028837\n",
       "1030    19.657686\n",
       "1031    16.382522\n",
       "1032    11.118284\n",
       "1033    17.341212\n",
       "1034    25.421418\n",
       "1035    21.573393\n",
       "1036    17.994438\n",
       "1037    26.410186\n",
       "1038    11.295101\n",
       "1039    21.081191\n",
       "1040    20.438549\n",
       "1041     6.281102\n",
       "1042    27.666490\n",
       "1043    10.640916\n",
       "Name: mape, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature_[all_feature_['datetime']>=pd.to_datetime('2025-01-10 00:00:00')]['mape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature.to_excel(r'Target_{}.xlsx'.format('2025-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict.to_excel(r'return.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = lstm_data[lstm_data.hour == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11.groupby('datetime').min().offered.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建示例数据\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_predict['datetime'],  # 将 '日期' 列作为 X 轴\n",
    "    y=all_predict['offered'],  # 将 '销售额' 列作为 Y 轴\n",
    "    mode='lines+markers',  # 折线 + 数据点\n",
    "    name='真实值'  # 图例名称\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_predict['datetime'],  # 将 '日期' 列作为 X 轴\n",
    "    y=all_predict['predict'],  # 将 '销售额' 列作为 Y 轴\n",
    "    mode='lines+markers',  # 折线 + 数据点\n",
    "    name='预测值'  # 图例名称\n",
    "))\n",
    "\n",
    "# 设置坐标轴名称\n",
    "fig.update_layout(\n",
    "    title='折线图示例',\n",
    "    xaxis_title='X轴',\n",
    "    yaxis_title='Y轴',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 可视化训练数据的预测结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot( all_predict['datetime'], all_predict['offered'] , label='Actual Data')\n",
    "plt.plot( all_predict['datetime'], all_predict['predict'], label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 预测未来数据\n",
    "future_steps = 30\n",
    "future_predictions = []\n",
    "\n",
    "last_sequence = X_train[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data.offered.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lstm_data.groupby(['year','month','day']).offered.sum()\n",
    "_.sort_values()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data[X_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 可视化训练数据的预测结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(now_lstm_data[['offered']][1000:].to_numpy(), label='Actual Data')\n",
    "plt.plot(return_y_pred, label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 预测未来数据\n",
    "future_steps = 30\n",
    "future_predictions = []\n",
    "\n",
    "last_sequence = X_train[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y =  model.predict( now_lstm_data[X_columns][:1000].to_numpy().reshape(-1, X_train.shape[1], 1))\n",
    "predict_y = scaler.inverse_transform(np.array(predict_y).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mape(now_lstm_data[['offered']][:1000].to_numpy(), predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ((np.abs((now_lstm_data[['offered']][1000:].to_numpy() - return_y_pred)/now_lstm_data[['offered']][1000:].to_numpy())))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_lstm_data[['offered']][1000:].to_numpy()[298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
