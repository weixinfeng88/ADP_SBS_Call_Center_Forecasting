{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25b664d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262/3049824355.py:77: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n",
      "/tmp/ipykernel_262/3049824355.py:77: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  data.datetime = data.datetime.dt.floor('T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 163ms/step - loss: 0.1285\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0404\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - loss: 0.0303\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 0.0295\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - loss: 0.0233\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0271\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0214\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0264\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0228\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0235\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0201\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0205\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0256\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0197\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - loss: 0.0243\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - loss: 0.0209\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - loss: 0.0206\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - loss: 0.0208\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0212\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0203\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step\n",
      "14.803346\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - loss: 0.0997\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0415\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0260\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0270\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0279\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - loss: 0.0236\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.0227\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0237\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0218\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0233\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - loss: 0.0209\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - loss: 0.0220\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.0222\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.0185\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - loss: 0.0220\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0237\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0185\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0226\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0205\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.0205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "15.643311\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "33206.383\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - loss: 0.2774\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0452\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - loss: 0.0388\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0271\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 0.0245\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0204\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0184\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0174\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0166\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0140\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0135\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - loss: 0.0146\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - loss: 0.0146\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0131\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - loss: 0.0158\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0157\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0135\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 0.0137\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0130\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - loss: 0.0115\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step\n",
      "14.174168\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - loss: 0.1817\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.0342\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - loss: 0.0356\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0234\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0225\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0229\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.0225\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - loss: 0.0198\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0211\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0191\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - loss: 0.0175\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0170\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0178\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.0179\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0157\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0159\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0159\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0190\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - loss: 0.0170\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0170\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step\n",
      "14.439988\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "15347.23\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 164ms/step - loss: 0.1733\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0347\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0187\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0193\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0202\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0155\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 0.0153\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0157\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0112\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0161\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - loss: 0.0164\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0152\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0156\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0135\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - loss: 0.0141\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 0.0146\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - loss: 0.0141\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - loss: 0.0142\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0116\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - loss: 0.0129\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "13.346781\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 0.1405\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0243\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - loss: 0.0261\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0221\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - loss: 0.0212\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0193\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - loss: 0.0180\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0198\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0164\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0185\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0169\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0168\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 0.0172\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0145\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - loss: 0.0142\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - loss: 0.0149\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0130\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0127\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0154\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0114\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "14.291816\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "15254.637\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - loss: 0.1047\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0272\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0145\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0169\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0148\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - loss: 0.0128\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0111\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0121\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - loss: 0.0109\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0127\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - loss: 0.0120\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - loss: 0.0116\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 0.0109\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.0110\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - loss: 0.0103\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - loss: 0.0098\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - loss: 0.0093\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - loss: 0.0096\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - loss: 0.0092\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - loss: 0.0092\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      "13.542293\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/lstm/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - loss: 0.0463\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - loss: 0.0167\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 0.0139\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0152\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - loss: 0.0128\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0125\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - loss: 0.0121\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.0136\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 0.0107\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.0127\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.0111\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 0.0123\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 0.0110\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 0.0118\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - loss: 0.0099\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - loss: 0.0106\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - loss: 0.0104\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - loss: 0.0094\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.0096\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - loss: 0.0104\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step\n",
      "13.527197\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "实际值中包含零，无法计算 MAPE。",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 401\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m         fill_na_predict[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 401\u001b[0m all_predict,all_feature\u001b[38;5;241m=\u001b[39m\u001b[43mTrain_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfill_na_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43munique_times_Midwest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m month_number\u001b[38;5;241m=\u001b[39mmonth_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "Cell \u001b[0;32mIn[16], line 338\u001b[0m, in \u001b[0;36mTrain_Model\u001b[0;34m(train_dataset, test_dataset, unique_times)\u001b[0m\n\u001b[1;32m    336\u001b[0m now_predict_2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(return_y_pred_2,columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    337\u001b[0m now_predict_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m now_time_list[\u001b[38;5;241m-\u001b[39mstart_time:]\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcalculate_mape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_y_pred_2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_mape(y_test, return_y_pred_1) \u001b[38;5;241m<\u001b[39m calculate_mape(y_test, return_y_pred_2):\n\u001b[1;32m    340\u001b[0m     all_predict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([now_predict_1,all_predict])\n",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m, in \u001b[0;36mcalculate_mape\u001b[0;34m(actual, predicted)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 避免除以零的情况\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(actual \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m实际值中包含零，无法计算 MAPE。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 计算 MAPE\u001b[39;00m\n\u001b[1;32m     39\u001b[0m mape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs((actual \u001b[38;5;241m-\u001b[39m predicted) \u001b[38;5;241m/\u001b[39m actual)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: 实际值中包含零，无法计算 MAPE。"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import datetime\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    计算 MAPE（平均绝对百分比误差）\n",
    "\n",
    "    参数:\n",
    "    \n",
    "    \n",
    "    actual (array-like): 实际值数组。\n",
    "    predicted (array-like): 预测值数组。\n",
    "\n",
    "    返回:\n",
    "    float: MAPE 值。\n",
    "    \"\"\"\n",
    "    # 将输入转换为 numpy 数组\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 避免除以零的情况\n",
    "    if np.any(actual == 0):\n",
    "        raise ValueError(\"实际值中包含零，无法计算 MAPE。\")\n",
    "    # 计算 MAPE\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mape\n",
    "def calculate_mape(actual, predicted):\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    mask = actual != 0\n",
    "    if not np.any(mask):\n",
    "        return np.nan  # Or return 0 or raise a different warning\n",
    "\n",
    "    mape = np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def ETL(month_list,data,holiday_data,full_dates,unique_times):\n",
    "    '''\n",
    "    month_list is the list you want to forecast one and half month in advanced, for example\n",
    "    , if you want to forecast May, 2025, please input ['2025-03-15'], also you can include multiple value in the list\n",
    "    , like ['2024-11-15','2024-12-15','2025-01-15','2025-02-15','2025-03-15'];\n",
    "    data is the source data in dataframe;\n",
    "    holiday_data is the holiday data in dataframe\n",
    "    full_dates are all the Business days you want to feed into Model for training;\n",
    "    unique_times are the intervals need to forecast for every day;\n",
    "    '''\n",
    "    for month_number in month_list:\n",
    "    #for month_number in ['2024-11-15','2024-12-15','2025-01-15','2025-02-15','2025-03-15']:\n",
    "    #for month_number in ['2024-11-15','2024-12-15','2025-01-15']:\n",
    "        #ETL Process\n",
    "        \n",
    "        #data=df.copy()\n",
    "        #pd.set_option('display.max_rows', 100)\n",
    "        data.columns = ['conversation_start_interval_tmst', 'Time', 'offered', 'actans',\n",
    "               'actabn', 'absActHt', 'absActSa', 'parent', 'child', 'fiscalDate',\n",
    "               'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']\n",
    "        \n",
    "        data = data[~data .offered.isna()]\n",
    "        \n",
    "        #data = data[data .offered != 0]\n",
    "        # Month one and half need to set up \n",
    "        data_=data.copy()\n",
    "        data = data[data.conversation_start_interval_tmst<=pd.to_datetime('{} 00:00:00'.format(month_number))]\n",
    "        holiday_data['~is_holiday'] = 0\n",
    "        Holiday_name = ['Christmas Day', 'Columbus Day',\n",
    "               'Independence Day', 'Labor Day', 'Martin Luther King Jr. Day',\n",
    "               'Memorial Day', \"New Year's Day\", \"Presidents' Day\", 'Thanksgiving Day',\n",
    "               'Veterans Day']\n",
    "        holiday_data['date'] = holiday_data.Date.dt.date\n",
    "        data['datetime'] = pd.to_datetime(data.conversation_start_interval_tmst)\n",
    "        data['date'] = data.datetime.dt.date\n",
    "        data.datetime = data.datetime.dt.floor('T')\n",
    "        data['datetime'] = data['datetime'].apply(\n",
    "            lambda x: x.ceil('30T') if x.minute == 29 else x\n",
    "        )\n",
    "        data['datetime'] = data['datetime'].apply(\n",
    "            lambda x: x.ceil('H') if x.minute == 59 else x\n",
    "        )\n",
    "        data['time'] = data.datetime.dt.time\n",
    "        data = data.sort_values('datetime').reset_index(drop = True)\n",
    "        data = data.sort_values(by='datetime')\n",
    "        date_column=data[['date',\n",
    "           'fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']]\n",
    "        date_column=date_column.set_index('date')\n",
    "        date_column = date_column[~date_column.index.duplicated(keep='first')]\n",
    "        date_column=date_column.sort_values('date')\n",
    "        # 获取所有唯一的时间点（如每天的09:00, 09:30等）\n",
    "        #unique_times = data['time'].unique()\n",
    "        \n",
    "        # 获取所有唯一的日期\n",
    "        unique_dates = data['date'].unique()\n",
    "        data = pd.merge(data,holiday_data,on = 'date',how = 'left').fillna(1)\n",
    "        data = data[data['~is_holiday'] == 1]\n",
    "        \n",
    "        holiday_datetime = holiday_data.date.to_numpy()\n",
    "        \n",
    "        start_time = 35+23 # 58 how many days in advance\n",
    "        end_time = start_time+36 #36 is the train length\n",
    "    \n",
    "        \n",
    "        add_column = ['actans','actabn','absActHt','absActSa']\n",
    "        \n",
    "        add_columns =  [f'actans_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                        [f'actabn_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                        [f'absActHt_{i}' for i in range(start_time+1,end_time+1) ] +\\\n",
    "                        [f'absActSa_{i}' for i in range(start_time+1,end_time+1) ]\n",
    "        \n",
    "        matrix = []\n",
    "        \n",
    "        # 遍历每个时间点\n",
    "        for time,time1,time2 in zip(unique_times,np.roll(unique_times, shift=-1),np.roll(unique_times, shift=1)):\n",
    "            # 过滤出当前时间点的数据\n",
    "            time_data = data[data['time'] == time].set_index('date')['offered']    \n",
    "            # full_dates #= pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "            full_dates = full_dates.difference(holiday_datetime)\n",
    "            #time_data = time_data.groupby(time_data.index).sum()\n",
    "            time_data = time_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "            shifted_data = pd.concat([time_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "            \n",
    "            \n",
    "            add_data = data[data['time'] == time].set_index('date')[add_column]\n",
    "            add_data = add_data.groupby(add_data.index).sum()\n",
    "            add_data = add_data.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "            add_data_ = pd.concat([add_data.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "            \n",
    "            \n",
    "            time_data1 = data[data['time'] == time1].set_index('date')['offered']    \n",
    "            time_data1 = time_data1.groupby(time_data1.index).sum()\n",
    "            time_data1 = time_data1.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "            shifted_data1 = pd.concat([time_data1.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "            \n",
    "            time_data2 = data[data['time'] == time2].set_index('date')['offered']    \n",
    "            time_data2 = time_data2.groupby(time_data2.index).sum()\n",
    "            time_data2 = time_data2.reindex(full_dates)  # 重新索引，缺失数据用NaN填充    \n",
    "            shifted_data2 = pd.concat([time_data2.shift(i) for i in range(start_time,end_time)], axis=1)\n",
    "            df=time_data.to_frame(name='offer')\n",
    "            df_last_year = df.copy()\n",
    "            df_last_year.index = df_last_year.index + pd.DateOffset(years=1)\n",
    "            df_last_year = df_last_year.rename(columns={'offer': 'offered_last_year'})\n",
    "            df_last_two_year = df.copy()\n",
    "            df_last_two_year.index = df_last_two_year.index + pd.DateOffset(years=2)\n",
    "            df_last_two_year = df_last_two_year.rename(columns={'offer': 'offered_last_two_year'})        \n",
    "            # Step 3: Join on index\n",
    "            result = df.join(df_last_year, how='left').join(df_last_two_year,how='left')\n",
    "            year_data=result[['offered_last_year','offered_last_two_year']]\n",
    "            #year_data = time_data.shift(251)\n",
    "            # 将当前时间点、前半小时和后半小时的数据拼接\n",
    "            combined_data = pd.concat([time_data,add_data_, shifted_data,shifted_data1,shifted_data2,year_data], axis=1)\n",
    "            combined_data = combined_data.merge(date_column, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "            # 创建日期+时间列\n",
    "            #datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in full_dates]).reshape(-1, 1)\n",
    "            datetime_column = np.array([pd.Timestamp(date) + pd.Timedelta(hours=time.hour, minutes=time.minute) for date in combined_data.index]).reshape(-1, 1)\n",
    "            # 将日期+时间列添加到数据中\n",
    "            combined_data_with_datetime = np.hstack([datetime_column,combined_data.to_numpy()])\n",
    "            \n",
    "            # 将结果存入矩阵\n",
    "            matrix.append(combined_data_with_datetime)\n",
    "        \n",
    "        max_rows = max(arr.shape[0] for arr in matrix)\n",
    "        \n",
    "        # 将每个时间点的数据填充到最大行数\n",
    "        for i in range(len(matrix)):\n",
    "            if matrix[i].shape[0] < max_rows:\n",
    "                padding = np.full((max_rows - matrix[i].shape[0], 11), np.nan)  # 用NaN填充（10列数据 + 1列时间）\n",
    "                matrix[i] = np.vstack([matrix[i], padding])\n",
    "        \n",
    "        # 将矩阵堆叠成一个大的二维数组\n",
    "        matrix = np.vstack(matrix)\n",
    "        \n",
    "        matrix = pd.DataFrame(matrix,columns = ['datetime','offered'] +add_columns +  [f'freq_{i}' for i in range(start_time+1,end_time+1) ]\\\n",
    "                              + [f'freq_last{i}' for i in range(start_time+1,end_time+1) ] + \\\n",
    "                              [f'freq_next{i}' for i in range(start_time+1,end_time+1) ]+ \\\n",
    "                               ['years_data_1','years_data_2','fiscalYear', 'ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW']).sort_values('datetime').reset_index(drop = True)\n",
    "        \n",
    "        matrix['hour'] = matrix.datetime.map(lambda x:x.hour)\n",
    "        matrix['minute'] = matrix.datetime.map(lambda x:x.minute)\n",
    "        matrix = matrix.sort_values('datetime').reset_index(drop = True)\n",
    "        \n",
    "        read_data = matrix.iloc[:]\n",
    "        \n",
    "        read_data = read_data[~read_data.offered.isna()].reset_index(drop = True)\n",
    "        # Quarter: 'Q1' → 1, ..., 'Q4' → 4\n",
    "        quarter_map = {'Q1': 1, 'Q2': 2, 'Q3': 3, 'Q4': 4}\n",
    "        read_data['ficalQuarter'] = read_data['ficalQuarter'].map(quarter_map)\n",
    "        year_map={'FY 2021':1 ,'FY 2022':2,'FY 2023':3,'FY 2024':4, 'FY 2025':5}\n",
    "        read_data['fiscalYear']=read_data['fiscalYear'].map(year_map)\n",
    "        # Month: 'January' → 1, ..., 'December' → 12\n",
    "        month_map = {\n",
    "            'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "            'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "            'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "        }\n",
    "        read_data['fiscalMonth'] = read_data['fiscalMonth'].map(month_map)\n",
    "        \n",
    "        # Day of week: 'Monday' → 0, ..., 'Sunday' → 6\n",
    "        dow_map = {\n",
    "            'Monday': 0, 'Tuesday': 1, 'Wednesday': 2,\n",
    "            'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "        }\n",
    "        read_data['DOW'] = read_data['DOW'].map(dow_map)\n",
    "        read_data['fiscalWeek']=read_data['fiscalWeek'].apply(lambda x:pd.to_numeric(x))\n",
    "        # Step 2: Apply cyclical encoding\n",
    "        \n",
    "        read_data['ficalQuarter_sin'] = np.sin(2 * np.pi * read_data['ficalQuarter'] / 4)\n",
    "        read_data['ficalQuarter_cos'] = np.cos(2 * np.pi * read_data['ficalQuarter'] / 4)\n",
    "        \n",
    "        read_data['fiscalMonth_sin'] = np.sin(2 * np.pi * read_data['fiscalMonth'] / 12)\n",
    "        read_data['fiscalMonth_cos'] = np.cos(2 * np.pi * read_data['fiscalMonth'] / 12)\n",
    "        \n",
    "        read_data['fiscalWeek_sin'] = np.sin(2 * np.pi * read_data['fiscalWeek'] / 52)\n",
    "        read_data['fiscalWeek_cos'] = np.cos(2 * np.pi * read_data['fiscalWeek'] / 52)\n",
    "        \n",
    "        read_data['DOW_sin'] = np.sin(2 * np.pi * read_data['DOW'] / 7)\n",
    "        read_data['DOW_cos'] = np.cos(2 * np.pi * read_data['DOW'] / 7)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Step 3: Drop original raw categorical time features\n",
    "        \n",
    "        read_data.drop(['ficalQuarter', 'fiscalMonth', 'fiscalWeek', 'DOW'], axis=1, inplace=True)\n",
    "        fill_na = read_data.iloc[:,1:].apply(lambda x:pd.to_numeric(x))\n",
    "    \n",
    "        fill_na = fill_na.interpolate(method='linear')\n",
    "        \n",
    "        fill_na['datetime'] = read_data.datetime\n",
    "        fill_na['date'] = fill_na.datetime.dt.date\n",
    "        fill_na = fill_na.dropna().reset_index(drop = True)\n",
    "    return fill_na\n",
    "\n",
    "def Train_Model(train_dataset,test_dataset,unique_times):\n",
    "    all_feature = pd.DataFrame()\n",
    "    all_predict=pd.DataFrame()\n",
    "    for (hour,minute) in [(i.hour,i.minute) for i in unique_times]:\n",
    "        now_lstm_data = train_dataset[(train_dataset.hour == hour)&(train_dataset.minute == minute)]\n",
    "        now_fill_na_predict = test_dataset[(test_dataset.hour == hour)&(test_dataset.minute == minute)]    \n",
    "        now_time_list = now_lstm_data.datetime.to_numpy()\n",
    "        cols=  ['offered', 'datetime'] + [col for col in now_lstm_data.columns if col not in ['offered', 'datetime','date']]\n",
    "        cols_=[col for col in now_lstm_data.columns if col not in ['offered', 'datetime','date']]\n",
    "        date_cols=['fiscalYear','ficalQuarter_sin','ficalQuarter_cos'\n",
    "              ,'fiscalMonth_sin','fiscalMonth_cos','fiscalWeek_sin','DOW_cos','DOW_sin','hour','minute','datetime','date','offered']\n",
    "        cal_cols=['fiscalYear','ficalQuarter_sin','ficalQuarter_cos'\n",
    "              ,'fiscalMonth_sin','fiscalMonth_cos','fiscalWeek_sin','DOW_cos','DOW_sin']\n",
    "        cols_offered=[col for col in now_lstm_data.columns if col not in date_cols]\n",
    "\n",
    "        now_lstm_data = now_lstm_data.reset_index(drop = True)\n",
    "        # 归一化数据\n",
    "        \n",
    "        # Separate the parts\n",
    "        to_scale_df = now_lstm_data[cols_offered]\n",
    "        keep_df = now_lstm_data[cal_cols]\n",
    "        scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler_y.fit_transform(now_lstm_data[['offered']])\n",
    "        scaled_array = scaler_x.fit_transform(to_scale_df)\n",
    "        scaled_df = pd.DataFrame(scaled_array, columns=cols_offered)\n",
    "        # Concatenate back together\n",
    "        X_combined = pd.concat([keep_df.reset_index(drop=True), scaled_df.reset_index(drop=True)], axis=1)\n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "        X_train_1, y_train =X_combined [:-start_time] , scaled_data[:-start_time]\n",
    "        X_test_1, y_test =X_combined[-start_time:]  , scaled_data[-start_time:]\n",
    "        X_train_1 = np.array(X_train_1)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_1 = np.array(X_test_1)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1)\n",
    "        X_train_1 = X_train_1.astype(np.float32)\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        X_test_1 = X_test_1.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "        # 构建LSTM模型\n",
    "        model_1 = Sequential()\n",
    "        model_1.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_1.add(Dense(25))\n",
    "        model_1.add(LSTM(20, return_sequences=False))\n",
    "        model_1.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_1.fit(X_train_1, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_1 = model_1.predict(X_train_1)\n",
    "        train_predict_1 = scaler_y.inverse_transform(train_predict_1)\n",
    "        True_data_1 = scaler_y.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_1, True_data_1))\n",
    "        return_y_1 =  model_1.predict(X_test_1)\n",
    "        return_y_pred_1 = scaler_y.inverse_transform(np.array(return_y_1).reshape(-1, 1))\n",
    "        now_predict_1 = pd.DataFrame(return_y_pred_1,columns = ['predict'])\n",
    "        now_predict_1['datetime'] = now_time_list[-start_time:]\n",
    "        now_lstm_data = now_lstm_data[['offered'] +cols_].reset_index(drop = True)        \n",
    "        # 设置时间步长\n",
    "        time_step = start_time\n",
    "        # 创建训练数据集\n",
    "        X_train_2 = X_combined[:-start_time]\n",
    "        X_test_2 = X_combined[-start_time:]\n",
    "        y_train = scaled_data[:-start_time]\n",
    "        y_test = scaled_data[-start_time:]\n",
    "        \n",
    "        X_train_2 = np.array(X_train_2)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test_2 = np.array(X_test_2)\n",
    "        y_test = np.array(y_test)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], X_test_2.shape[1], 1)\n",
    "    \n",
    "        # 重塑输入数据为 [samples, time steps, features] 格式\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], X_train_2.shape[1], 1)\n",
    "        X_train_2 = X_train_2.astype(np.float32)\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        X_test_2 = X_test_2.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "        # 构建LSTM模型\n",
    "        model_2 = Sequential()\n",
    "        model_2.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "        model_2.add(LSTM(50, return_sequences=False))\n",
    "        model_2.add(Dense(25))\n",
    "        model_2.add(Dense(1))\n",
    "    \n",
    "        # 编译模型\n",
    "        model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model_2.fit(X_train_2, y_train, batch_size=32, epochs=20)\n",
    "        train_predict_2 = model_2.predict(X_train_2)\n",
    "        train_predict_2 = scaler_y.inverse_transform(train_predict_2)\n",
    "        True_data_2 = scaler_y.inverse_transform(y_train.reshape(-1, 1))\n",
    "        print(calculate_mape(train_predict_2, True_data_2))\n",
    "        return_y_2 =  model_2.predict(X_test_2)\n",
    "        return_y_pred_2 = scaler_y.inverse_transform(np.array(return_y_2).reshape(-1, 1))\n",
    "        now_predict_2 = pd.DataFrame(return_y_pred_2,columns = ['predict'])\n",
    "        now_predict_2['datetime'] = now_time_list[-start_time:]\n",
    "        print(calculate_mape(y_test, return_y_pred_2))\n",
    "        if calculate_mape(y_test, return_y_pred_1) < calculate_mape(y_test, return_y_pred_2):\n",
    "            all_predict = pd.concat([now_predict_1,all_predict])\n",
    "            future_X = now_fill_na_predict[cols_].to_numpy()\n",
    "            future_data = scaler_y.inverse_transform(np.array(\\\n",
    "                            model_1.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        else:\n",
    "            all_predict = pd.concat([now_predict_2,all_predict])\n",
    "            future_X = now_fill_na_predict[cols_].to_numpy()\n",
    "            future_data = scaler_y.inverse_transform(np.array(\\\n",
    "                            model_2.predict(future_X.reshape(future_X.shape[0], future_X.shape[1], 1))).reshape(-1, 1))\n",
    "            future_data = pd.DataFrame(future_data,columns = ['predict'])\n",
    "            all_feature = pd.concat([all_feature,future_data])\n",
    "        #print('Interval Hour:{}, Minute:{} has been trained'.format(hour,minute)\n",
    "    all_predict = pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    all_predict = all_predict.sort_values('datetime').reset_index(drop = True)\n",
    "    #all_feature.to_excel(r'return_month_{}.xlsx'.format(month_number))\n",
    "    all_predict['diff'] = (all_predict.predict - all_predict.offered)\n",
    "    all_predict['hour'] = all_predict.datetime.dt.hour\n",
    "    pd.merge(all_predict,lstm_data[['offered','datetime']],on = ['datetime'])\n",
    "    calculate_mape(all_predict.offered,all_predict.predict)\n",
    "    \n",
    "    time_ = []\n",
    "    for (hour,minute) in fill_na_predict[['hour','minute']].drop_duplicates().to_numpy()[:]:\n",
    "        now_fill_na_predict = fill_na_predict[(fill_na_predict.hour == hour)&(fill_na_predict.minute == minute)]\n",
    "        time_.append(now_fill_na_predict.datetime.to_numpy())\n",
    "    all_feature['datetime'] =  np.hstack(time_)\n",
    "    all_feature.predict = all_feature.predict.map(lambda x:max(0,x))\n",
    "    data_['conversation_start_interval_tmst']=pd.to_datetime(data_['conversation_start_interval_tmst'])\n",
    "    all_feature['datetime']=pd.to_datetime(all_feature['datetime'])\n",
    "    all_feature = pd.merge(all_feature,data_[['offered','conversation_start_interval_tmst']],left_on = ['datetime'],right_on=['conversation_start_interval_tmst'])\n",
    "    all_feature = all_feature.sort_values('datetime').reset_index(drop = True)\n",
    "    all_feature['mape']=np.abs(all_feature['offered']-all_feature['predict'])/(all_feature['offered'])*100\n",
    "    return all_predict, all_feature\n",
    "\n",
    "data = pd.read_excel('/workspace/ADP_SBS_Call_Center_Forecasting/Data/MidWest_WFM_Stat_2025_05_16.xlsx')\n",
    "data_=data.copy()\n",
    "holiday_data = pd.read_excel(r'/workspace/ADP_SBS_Call_Center_Forecasting/Data/holiday.xlsx')\n",
    "month_list=['2025-02-15']\n",
    "unique_dates = data['conversation_start_interval_tmst'].dt.date.unique()\n",
    "start_time = 35 + 23\n",
    "end_time = start_time+36 #36 is the train length\n",
    "full_dates_train = pd.date_range(start=unique_dates.min(), end=unique_dates.max(), freq='B')  # 仅工作日\n",
    "full_dates_test = pd.date_range(start=unique_dates.min(), end=unique_dates.max()+ pd.offsets.BDay(start_time), freq='B')  # 仅工作日\n",
    "unique_times_Midwest = [ datetime.time(10, 0),\n",
    "datetime.time(10, 30), datetime.time(11, 0), datetime.time(11, 30),\n",
    "datetime.time(12, 0), datetime.time(12, 30), datetime.time(13, 0),\n",
    "datetime.time(13, 30), datetime.time(14, 0), datetime.time(14, 30),\n",
    "datetime.time(15, 0), datetime.time(15, 30), datetime.time(16, 0),\n",
    "datetime.time(16, 30), datetime.time(17, 0), datetime.time(17, 30),datetime.time(18,0),datetime.time(18,30),datetime.time(19,0),datetime.time(19,30),datetime.time(20,0),datetime.time(20,30)\n",
    "]\n",
    "fill_na=ETL(month_list,data,holiday_data,full_dates_train,unique_times_Midwest)\n",
    "fill_na_predict=ETL(month_list,data,holiday_data,full_dates_test,unique_times_Midwest)\n",
    "lstm_data = fill_na.dropna(axis = 0) \n",
    "lstm_data\n",
    "    \n",
    "for i in lstm_data.columns:\n",
    "    try:\n",
    "        fill_na_predict[i]\n",
    "    except:\n",
    "        fill_na_predict[i] = 0\n",
    "all_predict,all_feature=Train_Model(lstm_data,fill_na_predict,unique_times_Midwest)\n",
    "month_number=month_list[-1]\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "all_predict.to_excel(f'workspace/Midwest_CS_Test_Improved_{month_number}_{timestamp}.xlsx')\n",
    "all_predict.to_excel(f'workspace/Midwest_CS_Target_Improved_{month_number}_{timestamp}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55ea24-02c2-4ddb-94ad-ae953922b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "lstm_data\n",
    "calender_cols=['fiscalYear','ficalQuarter_sin','ficalQuarter_cos'\n",
    "              ,'fiscalMonth_sin','fiscalMonth_cos','fiscalWeek_sin','DOW_cos','DOW_sin','hour','minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f7343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a6e783-73c6-4316-8c58-af81843493ea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscaled_data\u001b[49m[cols_offered]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_data' is not defined"
     ]
    }
   ],
   "source": [
    "scaled_data[cols_offered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b845c9-3970-4e4e-9c57-f6aa28d4be8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6f641-48f2-443c-8335-99a875aca885",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/workspace/ADP_SBS_Call_Center_Forecasting/Data/MidWest_WFM_Stat_2025_05_16.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582b84b-b479-4f02-a00d-c862196932fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = data['conversation_start_interval_tmst'].dt.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab6366-0b0f-46c9-bfcb-aef4f0db965d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
